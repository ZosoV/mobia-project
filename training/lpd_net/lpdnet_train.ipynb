{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License Plate Detection Model for Mobia\n",
    "\n",
    "The following  notebook is based on the NVIDIA Developer Technical Blog [Creating a Real-Time License Plate Detection and Recognition App](https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app/) by [Yue Zhu](https://developer.nvidia.com/blog/author/tylerz/), [Morgan Huang](https://developer.nvidia.com/blog/author/morganh/) and [Fei Chen](https://developer.nvidia.com/blog/author/fechen/).\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained resnet18 model and train a ResNet-18 DetectNet_v2 model on the KITTI dataset\n",
    "* Prune the trained detectnet_v2 model\n",
    "* Retrain the pruned model to recover lost accuracy\n",
    "* Export the pruned model\n",
    "* Quantize the pruned model using QAT\n",
    "* Run Inference on the trained model\n",
    "* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n",
    "* Run inference on the exported. etlt model to verify deployment using TensorRT\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Object Detection using DetectNet_v2 in the Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Install the requirements and the TAO launcher](#head-1)\n",
    "    1. [Install Anaconda](#head-1-1)\n",
    "    1. [Install NVIDIA Container Toolkit](#head-1-2)\n",
    "    2. [Install Tao Launcher](#head-1-3)\n",
    "1. [Prepare dataset and pre-trained model](#head-2)\n",
    "    1. [Download the dataset](#head-2-1)\n",
    "    1. [Resize images and labels](#head-2-2)\n",
    "    1. [Split data](#head-2-3)\n",
    "    2. [Download pre-trained model](#head-2-4)\n",
    "2. [Provide training specification](#head-3)\n",
    "3. [Run TAO training](#head-4)\n",
    "4. [Evaluate trained models](#head-5)\n",
    "5. [Prune trained models](#head-6)\n",
    "6. [Retrain pruned models](#head-7)\n",
    "7. [Evaluate retrained model](#head-8)\n",
    "8. [Visualize inferences](#head-9)\n",
    "9. [Model Export](#head-10)\n",
    "    1. [Int8 Optimization](#head-10-1)\n",
    "    2. [Generate TensorRT engine](#head-10-2)\n",
    "10. [Verify Deployed Model](#head-11)\n",
    "    1. [Inference using TensorRT engine](#head-11-1)\n",
    "11. [QAT workflow](#head-12)\n",
    "    1. [Convert pruned model to QAT and retrain](#head-12-1)\n",
    "    2. [Evaluate QAT converted model](#head-12-2)\n",
    "    3. [Export QAT trained model to int8](#head-12-3)\n",
    "    4. [Evaluate a QAT trained model using the exported TensorRT engine](#head-12-4)\n",
    "    5. [Inference using QAT engine](#head-12-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "To start with this notebook we have to set local and Tao container environment variables. To do that, we use a same-name folder distribution tree for local machine and Tao Container. We will used the prefix `LOCAL` for local directories and `INT` for directories from Tao container. In the following cell, the environment variables are setting up. \n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "### Project Directory Tree for the License Plate Detection (LPDNet)\n",
    "\n",
    "```\n",
    "└── lpd_net\n",
    "    ├── data\n",
    "    |   ├── benchmark\n",
    "    |   ├── lpd\n",
    "    |   └── lpd_tfrecord\n",
    "    ├── experiments\n",
    "    |   ├── experiment_test\n",
    "    |   └── lpdnet_vunpruned_v1.0\n",
    "    ├── specs\n",
    "    └── lpd_prepare_data.py\n",
    "```\n",
    "\n",
    "Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/experiments`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$INT_MAIN_DIR` or `$INT_DATA_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n",
    "\n",
    "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables:\n",
      "\n",
      "env: KEY=tlt_encode\n",
      "env: NUM_GPUS=1\n",
      "\n",
      "Internal Paths for Tao Workspace:\n",
      "\n",
      "env: INT_MAIN_DIR=/workspace/tao-experiments/lpd_net\n",
      "env: INT_DATA_DIR=/workspace/tao-experiments/lpd_net/data\n",
      "env: INT_SPECS_DIR=/workspace/tao-experiments/lpd_net/specs\n",
      "\n",
      "Paths for local machine:\n",
      "\n",
      "LOCAL_PROJECT_DIR: /home/pepeleduin/mobia/mobia-project/training/lpd_net\n",
      "LOCAL_DATA_DIR: /home/pepeleduin/mobia/mobia-project/training/lpd_net/data\n",
      "LOCAL_EXPERIMENT_DIR: /home/pepeleduin/mobia/mobia-project/training/lpd_net/experiments\n",
      "LOCAL_SPECS_DIR: /home/pepeleduin/mobia/mobia-project/training/lpd_net/specs\n"
     ]
    }
   ],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "!printf \"Environment Variables:\\n\\n\"\n",
    "%env KEY=tlt_encode\n",
    "%env NUM_GPUS=1\n",
    "!printf \"\\nInternal Paths for Tao Workspace:\\n\\n\" \n",
    "%env INT_MAIN_DIR=/workspace/tao-experiments/lpd_net\n",
    "%env INT_DATA_DIR=/workspace/tao-experiments/lpd_net/data\n",
    "%env INT_SPECS_DIR=/workspace/tao-experiments/lpd_net/specs\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/detectnet_v2\n",
    "# !PLEASE MAKE SURE TO UPDATE THIS PATH!.\n",
    "\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = os.getcwd()\n",
    "\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"experiments\"\n",
    ")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "\n",
    "# Showing list of specification files.\n",
    "# !ls -rlt $LOCAL_SPECS_DIR\n",
    "!printf \"\\nPaths for local machine:\\n\\n\" \n",
    "!echo LOCAL_PROJECT_DIR:    $LOCAL_PROJECT_DIR\n",
    "!echo LOCAL_DATA_DIR:       $LOCAL_DATA_DIR\n",
    "!echo LOCAL_EXPERIMENT_DIR: $LOCAL_EXPERIMENT_DIR\n",
    "!echo LOCAL_SPECS_DIR:      $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. For more information please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide.\n",
    "\n",
    "When running this cell on AWS, update the drive_map entry with the dictionary defined below, so that you don't have permission issues when writing data into folders created by the TAO docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": os.environ[\"INT_MAIN_DIR\"]\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"INT_SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/pepeleduin/mobia/mobia-project/training/lpd_net\",\n",
      "            \"destination\": \"/workspace/tao-experiments/lpd_net\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/pepeleduin/mobia/mobia-project/training/lpd_net/specs\",\n",
      "            \"destination\": \"/workspace/tao-experiments/lpd_net/specs\"\n",
      "        }\n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Install the requirements and the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "_Note: Please, ensure to follow the instructions on [README.md](README.md) before continue._\n",
    "\n",
    "TAO Toolkit recommends users to run the TAO launcher in a virtual env with Python version between 3.6.9 and 3.8. In this project, we use Anaconda to manage the virtual environment and we need to the NVIDIA Container Toolkit.\n",
    "\n",
    "### 1.1 Installing on Anaconda <a class=\"anchor\" id=\"head-1-1\"></a>\n",
    "We consider that you already activate the environment with Conda.\n",
    "\n",
    "Requirements:\n",
    "* Python version >=3.6.9 < 3.8 (Already Installed)\n",
    "* Jupyter Lab (Already Installed)\n",
    "* OpenCV\n",
    "* Pillow\n",
    "* Pip (Already Installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in /home/pepeleduin/anaconda3/envs/mobia/lib/python3.7/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/pepeleduin/anaconda3/envs/mobia/lib/python3.7/site-packages (from opencv-python) (1.17.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: Pillow in /home/pepeleduin/anaconda3/envs/mobia/lib/python3.7/site-packages (8.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Install [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) <a class=\"anchor\" id=\"head-1-2\"></a>\n",
    "Requirements:\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.13\n",
      "Docker version 20.10.14, build a224086\n",
      "ii  nvidia-container-toolkit              1.9.0-1                          amd64        NVIDIA container runtime hook\n",
      "ii  nvidia-docker2                        2.10.0-1                         all          nvidia-docker CLI wrapper\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n"
     ]
    }
   ],
   "source": [
    "# Check Requirements\n",
    "!python --version \n",
    "!docker --version\n",
    "!dpkg -l | grep -e nvidia-container-toolkit -e nvidia-docker\n",
    "!nvidia-smi | grep CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Install Tao Launcher <a class=\"anchor\" id=\"head-1-3\"></a>\n",
    "The TAO launcher is a python package distributed as a Python wheel listed in the `nvidia-pyindex` Python index. You may install the Tao launcher by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/zosov/anaconda3/lib/python3.9/site-packages (1.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-tao in /home/zosov/anaconda3/lib/python3.9/site-packages (0.1.21)\n",
      "Requirement already satisfied: docker-pycreds==0.4.0 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (0.4.0)\n",
      "Requirement already satisfied: urllib3==1.25.10 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (1.25.10)\n",
      "Requirement already satisfied: chardet==3.0.4 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (3.0.4)\n",
      "Requirement already satisfied: docker==4.3.1 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (4.3.1)\n",
      "Requirement already satisfied: certifi==2020.6.20 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (2020.6.20)\n",
      "Requirement already satisfied: idna==2.10 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (2.10)\n",
      "Requirement already satisfied: tabulate==0.8.7 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (0.8.7)\n",
      "Requirement already satisfied: six==1.15.0 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (1.15.0)\n",
      "Requirement already satisfied: requests==2.24.0 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (2.24.0)\n",
      "Requirement already satisfied: websocket-client==0.57.0 in /home/zosov/anaconda3/lib/python3.9/site-packages (from nvidia-tao) (0.57.0)\n"
     ]
    }
   ],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher wheel.\n",
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "dockers: ['nvidia/tao/tao-toolkit-tf', 'nvidia/tao/tao-toolkit-pyt', 'nvidia/tao/tao-toolkit-lm']\n",
      "format_version: 2.0\n",
      "toolkit_version: 3.22.02\n",
      "published_date: 02/28/2022\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "### 2.A Download the dataset <a class=\"anchor\" id=\"head-2-1\"></a>\n",
    "First, sync the [OpenALPR benchmarks](https://github.com/openalpr/benchmarks) inside `$LOCAL_DATA_DIR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks'...\n",
      "remote: Enumerating objects: 1752, done.\u001b[K\n",
      "remote: Total 1752 (delta 0), reused 0 (delta 0), pack-reused 1752\u001b[K\n",
      "Receiving objects: 100% (1752/1752), 187.98 MiB | 4.01 MiB/s, done.\n",
      "Resolving deltas: 100% (34/34), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openalpr/benchmarks $LOCAL_DATA_DIR/benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Resize images and labels <a class=\"anchor\" id=\"head-2-2\"></a>\n",
    "Next, use the script `lpd_prepare_data.py` and run the following command to resize images/labels with width=640 and height=480 located in `$LOCAL_DATA_DIR/benchmarks/endtoend/us`. After this step, the data is organized in `$LOCAL_DATA_DIR/lpd` as follows:\n",
    "```\n",
    "└──dataset root\n",
    "  ├── images\n",
    "  |   ├── 000000.jpg\n",
    "  |   ├── 000001.jpg\n",
    "  |   |       .\n",
    "  |   |       .\n",
    "  |   └── xxxxxx.jpg\n",
    "  ├── labels\n",
    "  |   ├── 000000.txt\n",
    "  |   ├── 000001.txt\n",
    "  |   |       .\n",
    "  |   |       .\n",
    "  |   └── xxxxxx.txt\n",
    "  └── kitti_seq_to_map.json\n",
    "```\n",
    "\n",
    "`kitti_seq_to_map.json` is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000155.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000165.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car8.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000170.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000023.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000043.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000151.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000173.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000044.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000128.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000096.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000141.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000091.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000168.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000167.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000175.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000199.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us2.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000062.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us5.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000127.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000025.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car2.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000031.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000020.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-0.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000194.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000077.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000164.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000177.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000052.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000029.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000166.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car17.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000100.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car18.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000129.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000101.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car3.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000088.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000084.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000115.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000131.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car7.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000113.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/33fa5185-0286-4e8f-b775-46162eba39d4.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000022.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000038.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000039.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car14.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000133.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000162.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000157.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000035.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000099.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-9.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000145.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000032.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000072.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us1.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000140.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000057.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car19.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000049.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us4.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000067.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us8.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000070.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000144.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us6.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000146.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000192.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000102.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000098.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000082.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000135.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000013.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000156.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000085.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000121.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000120.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000118.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000027.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000150.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000169.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000055.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-7.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000018.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000041.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car13.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000189.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/5b562a61-34ad-4f00-9164-d34abb7a38e4.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000073.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/c9368c55-210d-456c-a5ef-c310e60039ec.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/316b64c0-55bf-4079-a1c0-d93f461a576f.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000190.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000171.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000047.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000176.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000053.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000046.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/21d8c31d-3deb-494b-9c63-c0223306fd82.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000159.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000174.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000122.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/37170dd1-2802-4e38-b982-c5d07c64ff67.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/1e241dc8-8f18-4955-8988-03a0ab49f813.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000078.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000028.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000024.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000125.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000178.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000123.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000069.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000059.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000083.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000152.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000172.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000097.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000011.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car11.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000183.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000136.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000196.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car15.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car5.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000064.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000160.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000076.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000014.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000181.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000139.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000034.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car22.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000071.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000048.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000154.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000143.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000065.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000012.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000187.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000188.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000086.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000042.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000117.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000058.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-2.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/d4f79480-366a-40b6-ab2c-328bcba705b2.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000094.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000033.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000193.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000137.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/f0a3b8c0-198a-471b-9ca9-345c3dd28073.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/3850ba91-3c64-4c64-acba-0c46b61ec0da.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-1.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000017.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000037.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000126.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000090.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000051.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000179.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000147.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000191.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/12c6cb72-3ea3-49e7-b381-e0cdfc5e8960.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000068.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000036.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000186.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/22e54a62-57a8-4a0a-88c1-4b9758f67651.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000161.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-5.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000066.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000040.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000134.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000124.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000026.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us3.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car21.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000149.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000095.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/7fbfbe28-aecb-45be-bd05-7cf26acb3c5c.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000074.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/a03ced3f-5a97-4e75-8106-fabfd2b8b76e.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000130.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000138.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car6.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000148.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000119.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000106.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000050.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000080.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000010.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000045.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000087.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car16.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000021.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000195.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000163.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/f8fc5e59-9083-466b-ae3f-6b869a0b257b.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000056.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000016.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000197.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000063.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000061.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000142.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/e73fd200-7ba4-4725-9d1d-2ec710864df6.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car1.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000158.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car9-4.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us10.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000030.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000132.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000075.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000089.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car20.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/car12.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000060.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/wts-lg-000079.jpg and its label\n",
      "Resizing /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/benchmarks/endtoend/us/us7.jpg and its label\n",
      "\n",
      "Done. Resized images/labels are saved at /home/pepeleduin/mobia/mobia-project/training/lpd_net/data/lpd folder\n"
     ]
    }
   ],
   "source": [
    "!python lpd_prepare_data.py --input_dir $LOCAL_DATA_DIR/benchmarks/endtoend/us \\\n",
    "                            --output_dir $LOCAL_DATA_DIR/lpd \\\n",
    "                            --target_width  640 \\\n",
    "                            --target_height 480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Split data <a class=\"anchor\" id=\"head-2-3\"></a>\n",
    "\n",
    "A spec file called `SPECS_tfrecord.txt` located in `$INT_SPECS_DIR` splits the data. It points to the same path in local machine, but internal to Tao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitti_config {\n",
      "  root_directory_path: \"/workspace/tao-experiments/lpd_net/data/lpd/data\"\n",
      "  image_dir_name: \"image\"\n",
      "  label_dir_name: \"label\"\n",
      "  image_extension: \".jpg\"\n",
      "  partition_mode: \"random\"\n",
      "  num_partitions: 2\n",
      "  val_split: 20\n",
      "  num_shards: 4\n",
      "}\n",
      "\n",
      "image_directory_path: \"/workspace/tao-experiments/lpd_net/data/lpd/data\""
     ]
    }
   ],
   "source": [
    "!cat $LOCAL_SPECS_DIR/SPECS_tfrecord.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following paths must be changed in `SPECS_tfrecord.txt`: `root_directory_path`, `image_directory_path`. These paths should contain the output of the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/tao-experiments/lpd_net/data/lpd/data/\n"
     ]
    }
   ],
   "source": [
    "!echo $INT_DATA_DIR/lpd/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Later, split the data into two parts: 80% for the training set and 20% for the validation set. Run the following command to split the dataset randomly and generate tfrecords. The first that you will use `tao`, the command will start to download the `nvcr.io/nvidia/tao/tao-toolkit-tf` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-08 10:37:00,497 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-04-08 10:37:00,561 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.4-py3\n",
      "2022-04-08 10:37:00,604 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/pepeleduin/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "2022-04-08 15:37:06,956 [INFO] iva.detectnet_v2.dataio.build_converter: Instantiating a kitti converter\n",
      "2022-04-08 15:37:06,956 [INFO] root: Instantiating a kitti converter\n",
      "2022-04-08 15:37:06,956 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Creating output directory /workspace/tao-experiments/lpd_net/data/lpd_tfrecord\n",
      "2022-04-08 15:37:06,956 [INFO] root: Generating partitions\n",
      "2022-04-08 15:37:06,957 [INFO] iva.detectnet_v2.dataio.kitti_converter_lib: Num images in\n",
      "Train: 178\tVal: 44\n",
      "2022-04-08 15:37:06,957 [INFO] root: Num images in\n",
      "Train: 178\tVal: 44\n",
      "2022-04-08 15:37:06,957 [INFO] iva.detectnet_v2.dataio.kitti_converter_lib: Validation data in partition 0. Hence, while choosing the validationset during training choose validation_fold 0.\n",
      "2022-04-08 15:37:06,957 [INFO] root: Validation data in partition 0. Hence, while choosing the validationset during training choose validation_fold 0.\n",
      "2022-04-08 15:37:06,958 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 0, shard 0\n",
      "2022-04-08 15:37:06,958 [INFO] root: Writing partition 0, shard 0\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:161: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "2022-04-08 15:37:06,958 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:161: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/iva/detectnet_v2/dataio/kitti_converter_lib.py:297: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "2022-04-08 15:37:06,974 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 0, shard 1\n",
      "2022-04-08 15:37:06,974 [INFO] root: Writing partition 0, shard 1\n",
      "2022-04-08 15:37:06,981 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 0, shard 2\n",
      "2022-04-08 15:37:06,982 [INFO] root: Writing partition 0, shard 2\n",
      "2022-04-08 15:37:06,990 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 0, shard 3\n",
      "2022-04-08 15:37:06,990 [INFO] root: Writing partition 0, shard 3\n",
      "2022-04-08 15:37:06,999 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'lpd': 44\n",
      "\n",
      "2022-04-08 15:37:06,999 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 1, shard 0\n",
      "2022-04-08 15:37:06,999 [INFO] root: Writing partition 1, shard 0\n",
      "2022-04-08 15:37:07,035 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 1, shard 1\n",
      "2022-04-08 15:37:07,035 [INFO] root: Writing partition 1, shard 1\n",
      "2022-04-08 15:37:07,070 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 1, shard 2\n",
      "2022-04-08 15:37:07,071 [INFO] root: Writing partition 1, shard 2\n",
      "2022-04-08 15:37:07,113 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing partition 1, shard 3\n",
      "2022-04-08 15:37:07,113 [INFO] root: Writing partition 1, shard 3\n",
      "2022-04-08 15:37:07,165 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'lpd': 178\n",
      "\n",
      "2022-04-08 15:37:07,165 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Cumulative object statistics\n",
      "2022-04-08 15:37:07,165 [INFO] root: Cumulative object statistics\n",
      "2022-04-08 15:37:07,165 [INFO] root: {\n",
      "    \"lpd\": 222\n",
      "}\n",
      "2022-04-08 15:37:07,165 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'lpd': 222\n",
      "\n",
      "2022-04-08 15:37:07,165 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Class map. \n",
      "Label in GT: Label in tfrecords file \n",
      "b'lpd': b'lpd'\n",
      "2022-04-08 15:37:07,165 [INFO] root: Class map. \n",
      "Label in GT: Label in tfrecords file \n",
      "b'lpd': b'lpd'\n",
      "For the dataset_config in the experiment_spec, please use labels in the tfrecords file, while writing the classmap.\n",
      "\n",
      "2022-04-08 15:37:07,165 [INFO] root: For the dataset_config in the experiment_spec, please use labels in the tfrecords file, while writing the classmap.\n",
      "\n",
      "2022-04-08 15:37:07,165 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Tfrecords generation complete.\n",
      "2022-04-08 15:37:07,165 [INFO] root: TFRecords generation complete.\n",
      "2022-04-08 10:37:08,018 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao detectnet_v2 dataset_convert \\\n",
    "                -d $INT_SPECS_DIR/SPECS_tfrecord.txt \\\n",
    "                -o $INT_DATA_DIR/lpd_tfrecord/lpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D Download pre-trained model <a class=\"anchor\" id=\"head-2-4\"></a>\n",
    "Download the correct pretrained model from the NGC model registry for your experiment. Please note that for DetectNet_v2, the input is expected to be 0-1 normalized with input channels in RGB order. Therefore, for optimum results please download model templates from `nvidia/tao/pretrained_detectnet_v2`. The templates are now organized as version strings. For example, to download a resnet18 model suitable for detectnet please resolve to the ngc object shown as `nvidia/tao/pretrained_detectnet_v2:resnet18`. \n",
    "\n",
    "All other models are in BGR order expect input preprocessing with mean subtraction and input channels. Using them as pretrained weights may result in suboptimal performance.\n",
    "\n",
    "You may also use this notebook with the following purpose-built pretrained models \n",
    "* [PeopleNet](https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet)\n",
    "* [TrafficCamNet](https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet)\n",
    "* [DashCamNet](https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet)\n",
    "* [FaceDetect-IR](https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2022-04-08 10:37:33--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.64.174.22, 18.64.174.116, 18.64.174.58, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.64.174.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32588469 (31M) [application/zip]\n",
      "Saving to: ‘/home/pepeleduin/mobia/mobia-project/training/lpd_net/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  31.08M  4.21MB/s    in 7.6s    \n",
      "\n",
      "2022-04-08 10:37:41 (4.08 MB/s) - ‘/home/pepeleduin/mobia/mobia-project/training/lpd_net/ngccli/ngccli_cat_linux.zip’ saved [32588469/32588469]\n",
      "\n",
      "Archive:  /home/pepeleduin/mobia/mobia-project/training/lpd_net/ngccli/ngccli_cat_linux.zip\n",
      "  inflating: /home/pepeleduin/mobia/mobia-project/training/lpd_net/ngccli/ngc  \n",
      " extracting: /home/pepeleduin/mobia/mobia-project/training/lpd_net/ngccli/ngc.md5  \n"
     ]
    }
   ],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip  -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" \\\n",
    "        -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"),\n",
    "                                         os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| Versi | Accur | Epoch | Batch | GPU   | Memor | File  | Statu | Creat |\n",
      "| on    | acy   | s     | Size  | Model | y Foo | Size  | s     | ed    |\n",
      "|       |       |       |       |       | tprin |       |       | Date  |\n",
      "|       |       |       |       |       | t     |       |       |       |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| unpru | 98.0  | 120   | 1     | V100  | 67.5  | 67.53 | UPLOA | Nov   |\n",
      "| ned_v |       |       |       |       |       | MB    | D_COM | 24,   |\n",
      "| 2.0   |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| unpru | 98.0  | 120   | 1     | V100  | 85.8  | 85.83 | UPLOA | Aug   |\n",
      "| ned_v |       |       |       |       |       | MB    | D_COM | 24,   |\n",
      "| 1.0   |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| prune | 98.0  | 120   | 1     | V100  | 5.3   | 5.34  | UPLOA | Nov   |\n",
      "| d_v2. |       |       |       |       |       | MB    | D_COM | 24,   |\n",
      "| 0     |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| prune | 98.0  | 120   | 1     | V100  | 1.4   | 1.43  | UPLOA | Aug   |\n",
      "| d_v1. |       |       |       |       |       | MB    | D_COM | 24,   |\n",
      "| 0     |       |       |       |       |       |       | PLETE | 2021  |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "# List models available in the model registry.\n",
    "!ngc registry model list nvidia/tao/lpdnet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 85.86 MB in 29s, Download speed: 2.96 MB/s               \n",
      "----------------------------------------------------\n",
      "Transfer id: lpdnet_vunpruned_v1.0 Download status: Completed.\n",
      "Downloaded local path: /home/pepeleduin/mobia/mobia-project/training/lpd_net/experiments/lpdnet_vunpruned_v1.0\n",
      "Total files downloaded: 2 \n",
      "Total downloaded size: 85.86 MB\n",
      "Started at: 2022-04-08 10:37:57.303291\n",
      "Completed at: 2022-04-08 10:38:26.335331\n",
      "Duration taken: 29s\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/lpdnet:unpruned_v1.0 \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "drwx------ 2 pepeleduin pepeleduin 4096 Apr  8 10:38 lpdnet_vunpruned_v1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the download model status\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Tfrecords for the train datasets\n",
    "    * To use the newly generated tfrecords, update the dataset_config parameter in the spec file at `$SPECS_DIR/SPECS_train.txt` \n",
    "    * Update the fold number to use for evaluation. In case of random data split, please use fold `0` only\n",
    "    * For sequence-wise split, you may use any fold generated from the dataset convert tool\n",
    "* Pre-trained models\n",
    "* Augmentation parameters for on the fly data augmentation\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "# The MIT License (MIT)\n",
      "#\n",
      "# Copyright (c) 2019-2021 NVIDIA CORPORATION\n",
      "#\n",
      "# Permission is hereby granted, free of charge, to any person obtaining a\n",
      "# copy of this software and associated documentation files (the \"Software\"),\n",
      "# to deal in the Software without restriction, including without limitation\n",
      "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
      "# and/or sell copies of the Software, and to permit persons to whom the\n",
      "# Software is furnished to do so, subject to the following conditions:\n",
      "#\n",
      "# The above copyright notice and this permission notice shall be included in\n",
      "# all copies or substantial portions of the Software.\n",
      "#\n",
      "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n",
      "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
      "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
      "# DEALINGS IN THE SOFTWARE.\n",
      "################################################################################\n",
      "\n",
      "random_seed: 42\n",
      "dataset_config {\n",
      "  data_sources {\n",
      "    tfrecords_path: \"/workspace/tao-experiments/lpd_net/data/lpd_tfrecord/*\"\n",
      "    image_directory_path: \"/workspace/tao-experiments/lpd_net/data/lpd/data/\"\n",
      "  }\n",
      "  image_extension: \"jpg\"\n",
      "  target_class_mapping {\n",
      "    key: \"lpd\"\n",
      "    value: \"lpd\"\n",
      "  }\n",
      "  validation_fold: 0\n",
      "}\n",
      "augmentation_config {\n",
      "  preprocessing {\n",
      "    output_image_width: 640\n",
      "    output_image_height: 480\n",
      "    min_bbox_width: 1.0\n",
      "    min_bbox_height: 1.0\n",
      "    output_image_channel: 3\n",
      "  }\n",
      "  spatial_augmentation {\n",
      "    hflip_probability: 0.5\n",
      "    zoom_min: 1.0\n",
      "    zoom_max: 1.0\n",
      "    translate_max_x: 8.0\n",
      "    translate_max_y: 8.0\n",
      "  }\n",
      "  color_augmentation {\n",
      "    hue_rotation_max: 25.0\n",
      "    saturation_shift_max: 0.20000000298\n",
      "    contrast_scale_max: 0.10000000149\n",
      "    contrast_center: 0.5\n",
      "  }\n",
      "}\n",
      "postprocessing_config {\n",
      "  target_class_config {\n",
      "    key: \"lpd\"\n",
      "    value {\n",
      "      clustering_config {\n",
      "        coverage_threshold: 0.00499999988824\n",
      "        dbscan_eps: 0.20000000298\n",
      "        dbscan_min_samples: 0.0500000007451\n",
      "        minimum_bounding_box_height: 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "model_config {\n",
      "  pretrained_model_file: \"/workspace/tao-experiments/lpd_net/experiments/lpdnet_vunpruned_v1.0/usa_unpruned.tlt\"\n",
      "  num_layers: 18\n",
      "  use_batch_norm: true\n",
      "  objective_set {\n",
      "    bbox {\n",
      "      scale: 35.0\n",
      "      offset: 0.5\n",
      "    }\n",
      "    cov {\n",
      "    }\n",
      "  }\n",
      "  training_precision {\n",
      "    backend_floatx: FLOAT32\n",
      "  }\n",
      "  arch: \"resnet\"\n",
      "}\n",
      "evaluation_config {\n",
      "  validation_period_during_training: 10\n",
      "  first_validation_epoch: 1\n",
      "  minimum_detection_ground_truth_overlap {\n",
      "    key: \"lpd\"\n",
      "    value: 0.699999988079\n",
      "  }\n",
      "  evaluation_box_config {\n",
      "    key: \"lpd\"\n",
      "    value {\n",
      "      minimum_height: 10\n",
      "      maximum_height: 9999\n",
      "      minimum_width: 10\n",
      "      maximum_width: 9999\n",
      "    }\n",
      "  }\n",
      "  average_precision_mode: INTEGRATE\n",
      "}\n",
      "cost_function_config {\n",
      "  target_classes {\n",
      "    name: \"lpd\"\n",
      "    class_weight: 1.0\n",
      "    coverage_foreground_weight: 0.0500000007451\n",
      "    objectives {\n",
      "      name: \"cov\"\n",
      "      initial_weight: 1.0\n",
      "      weight_target: 1.0\n",
      "    }\n",
      "    objectives {\n",
      "      name: \"bbox\"\n",
      "      initial_weight: 10.0\n",
      "      weight_target: 10.0\n",
      "    }\n",
      "  }\n",
      "  enable_autoweighting: true\n",
      "  max_objective_weight: 0.999899983406\n",
      "  min_objective_weight: 9.99999974738e-05\n",
      "}\n",
      "training_config {\n",
      "  batch_size_per_gpu: 4\n",
      "  num_epochs: 120\n",
      "  enable_qat: False\n",
      "  learning_rate {\n",
      "    soft_start_annealing_schedule {\n",
      "      min_learning_rate: 5e-06\n",
      "      max_learning_rate: 5e-04\n",
      "      soft_start: 0.10000000149\n",
      "      annealing: 0.699999988079\n",
      "    }\n",
      "  }\n",
      "  regularizer {\n",
      "    type: L1\n",
      "    weight: 3.00000002618e-09\n",
      "  }\n",
      "  optimizer {\n",
      "    adam {\n",
      "      epsilon: 9.99999993923e-09\n",
      "      beta1: 0.899999976158\n",
      "      beta2: 0.999000012875\n",
      "    }\n",
      "  }\n",
      "  cost_scaling {\n",
      "    initial_exponent: 20.0\n",
      "    increment: 0.005\n",
      "    decrement: 1.0\n",
      "  }\n",
      "  checkpoint_interval: 10\n",
      "}\n",
      "bbox_rasterizer_config {\n",
      "  target_class_config {\n",
      "    key: \"lpd\"\n",
      "    value {\n",
      "      cov_center_x: 0.5\n",
      "      cov_center_y: 0.5\n",
      "      cov_radius_x: 0.40000000596\n",
      "      cov_radius_y: 0.40000000596\n",
      "      bbox_min_radius: 1.0\n",
      "    }\n",
      "  }\n",
      "  deadzone_radius: 0.400000154972\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat $LOCAL_SPECS_DIR/SPECS_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "\n",
    "*Note: The training may take hours to complete. Also, the remaining notebook, assumes that the training was done in single-GPU mode. When run in multi-GPU mode, please expect to update the pruning and inference steps with new pruning thresholds and updated parameters in the clusterfile.json accordingly for optimum performance.*\n",
    "\n",
    "*Detectnet_v2 now supports restart from checkpoint. In case the training job is killed prematurely, you may resume training from the closest checkpoint by simply re-running the **same** command line. Please do make sure to use the <u>**same number of GPUs**</u> when restarting the training.*\n",
    "\n",
    "*When running the training with NUM_GPUs>1, you may need to modify the `batch_size_per_gpu` and `learning_rate` to get similar mAP as a 1GPU training run. In most cases, scaling down the batch-size by a factor of NUM_GPU's or scaling up the learning rate by a factor of NUM_GPU's would be a good place to start.*\n",
    "\n",
    "Useful [link](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/detectnet_v2.html) to documentation for Tao Detectnet V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_NAME=lpd_net_test\n",
      "env: LOG_NAME=log_lpd_net_test01\n"
     ]
    }
   ],
   "source": [
    "# Set a env variable to define the name of the model\n",
    "%env MODEL_NAME=lpd_net_test\n",
    "%env LOG_NAME=log_lpd_net_test01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-08 10:45:02,664 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-04-08 10:45:02,727 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.4-py3\n",
      "2022-04-08 10:45:02,754 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/pepeleduin/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "2022-04-08 15:45:08,106 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/checkpoint_saver_hook.py:25: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.\n",
      "\n",
      "2022-04-08 15:45:08,210 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/checkpoint_saver_hook.py:25: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:69: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "2022-04-08 15:45:08,211 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:69: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:69: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "2022-04-08 15:45:08,211 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:69: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-04-08 15:45:08,220 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-04-08 15:45:08,220 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-04-08 15:45:08,607 [INFO] __main__: Loading experiment spec at /workspace/tao-experiments/lpd_net/specs/SPECS_train.txt.\n",
      "2022-04-08 15:45:08,608 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /workspace/tao-experiments/lpd_net/specs/SPECS_train.txt\n",
      "2022-04-08 15:45:08,678 [INFO] __main__: Cannot iterate over exactly 178 samples with a batch size of 4; each epoch will therefore take one extra step.\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2022-04-08 15:45:08,679 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2022-04-08 15:45:08,679 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-04-08 15:45:08,681 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-04-08 15:45:08,704 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-04-08 15:45:08,705 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-04-08 15:45:08,725 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-04-08 15:45:10,342 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-04-08 15:45:10,342 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-04-08 15:45:10,674 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2022-04-08 15:45:17,063 [INFO] iva.detectnet_v2.objectives.bbox_objective: Default L1 loss function will be used.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 480, 640)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 240, 320) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 240, 320) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 240, 320) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (Conv2D)        (None, 64, 120, 160) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 64, 120, 160) 256         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (Activation)    (None, 64, 120, 160) 0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (Conv2D)        (None, 64, 120, 160) 36928       block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Conv2D) (None, 64, 120, 160) 4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 120, 160) 256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 120, 160) 256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 120, 160) 0           block_1a_bn_2[0][0]              \n",
      "                                                                 block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (Activation)      (None, 64, 120, 160) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (Conv2D)        (None, 64, 120, 160) 36928       block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 120, 160) 256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (Activation)    (None, 64, 120, 160) 0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (Conv2D)        (None, 64, 120, 160) 36928       block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 120, 160) 256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 120, 160) 0           block_1b_bn_2[0][0]              \n",
      "                                                                 block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (Activation)      (None, 64, 120, 160) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (Conv2D)        (None, 128, 60, 80)  73856       block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 128, 60, 80)  512         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (Activation)    (None, 128, 60, 80)  0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (Conv2D)        (None, 128, 60, 80)  147584      block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Conv2D) (None, 128, 60, 80)  8320        block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 128, 60, 80)  512         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 128, 60, 80)  512         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 60, 80)  0           block_2a_bn_2[0][0]              \n",
      "                                                                 block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (Activation)      (None, 128, 60, 80)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (Conv2D)        (None, 128, 60, 80)  147584      block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 128, 60, 80)  512         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (Activation)    (None, 128, 60, 80)  0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (Conv2D)        (None, 128, 60, 80)  147584      block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 128, 60, 80)  512         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 128, 60, 80)  0           block_2b_bn_2[0][0]              \n",
      "                                                                 block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (Activation)      (None, 128, 60, 80)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (Conv2D)        (None, 256, 30, 40)  295168      block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 256, 30, 40)  1024        block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (Activation)    (None, 256, 30, 40)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (Conv2D)        (None, 256, 30, 40)  590080      block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Conv2D) (None, 256, 30, 40)  33024       block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 256, 30, 40)  1024        block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 256, 30, 40)  1024        block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 30, 40)  0           block_3a_bn_2[0][0]              \n",
      "                                                                 block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (Activation)      (None, 256, 30, 40)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (Conv2D)        (None, 256, 30, 40)  590080      block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 256, 30, 40)  1024        block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (Activation)    (None, 256, 30, 40)  0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (Conv2D)        (None, 256, 30, 40)  590080      block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 256, 30, 40)  1024        block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 30, 40)  0           block_3b_bn_2[0][0]              \n",
      "                                                                 block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (Activation)      (None, 256, 30, 40)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (Conv2D)        (None, 512, 30, 40)  1180160     block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 512, 30, 40)  2048        block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (Activation)    (None, 512, 30, 40)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (Conv2D)        (None, 512, 30, 40)  2359808     block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Conv2D) (None, 512, 30, 40)  131584      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 512, 30, 40)  2048        block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 512, 30, 40)  2048        block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 30, 40)  0           block_4a_bn_2[0][0]              \n",
      "                                                                 block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (Activation)      (None, 512, 30, 40)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (Conv2D)        (None, 512, 30, 40)  2359808     block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 512, 30, 40)  2048        block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (Activation)    (None, 512, 30, 40)  0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (Conv2D)        (None, 512, 30, 40)  2359808     block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 512, 30, 40)  2048        block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 30, 40)  0           block_4b_bn_2[0][0]              \n",
      "                                                                 block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (Activation)      (None, 512, 30, 40)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_bbox (Conv2D)            (None, 4, 30, 40)    2052        block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_cov (Conv2D)             (None, 1, 30, 40)    513         block_4b_relu[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,197,893\n",
      "Trainable params: 11,188,165\n",
      "Non-trainable params: 9,728\n",
      "__________________________________________________________________________________________________\n",
      "2022-04-08 15:45:17,097 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-04-08 15:45:17,097 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-04-08 15:45:17,097 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-04-08 15:45:17,097 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 12, io threads: 24, compute threads: 12, buffered batches: 4\n",
      "2022-04-08 15:45:17,097 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 178, number of sources: 1, batch size per gpu: 4, steps: 45\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "2022-04-08 15:45:17,136 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f4e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f4e0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:17,179 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f4e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f4e0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:17,198 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-04-08 15:45:17,428 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: True - shard 0 of 1\n",
      "2022-04-08 15:45:17,433 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-04-08 15:45:17,433 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f27b8efe940>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f27b8efe940>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:17,446 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f27b8efe940>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f27b8efe940>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:17,774 [INFO] __main__: Found 178 samples in training set\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "2022-04-08 15:45:17,868 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:89: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "2022-04-08 15:45:17,969 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:89: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "2022-04-08 15:45:17,983 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-04-08 15:45:18,134 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-04-08 15:45:18,142 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/model/detectnet_model.py:591: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "2022-04-08 15:45:18,145 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/model/detectnet_model.py:591: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "2022-04-08 15:45:19,359 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-04-08 15:45:19,359 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-04-08 15:45:19,359 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-04-08 15:45:19,360 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 12, io threads: 24, compute threads: 12, buffered batches: 4\n",
      "2022-04-08 15:45:19,360 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 44, number of sources: 1, batch size per gpu: 4, steps: 11\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f588>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:19,368 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f27c096f588>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:19,383 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-04-08 15:45:19,672 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: False - shard 0 of 1\n",
      "2022-04-08 15:45:19,676 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-04-08 15:45:19,676 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f277843d7f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f277843d7f0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:19,688 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f277843d7f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f277843d7f0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-04-08 15:45:19,899 [INFO] __main__: Found 44 samples in validation set\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/validation_hook.py:40: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "2022-04-08 15:45:20,375 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/validation_hook.py:40: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "2022-04-08 15:45:21,290 [INFO] __main__: Checkpoint interval: 10\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:109: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "2022-04-08 15:45:21,291 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:109: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "2022-04-08 15:45:21,291 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "2022-04-08 15:45:21,291 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "2022-04-08 15:45:21,292 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:59: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
      "\n",
      "2022-04-08 15:45:21,293 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:59: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:60: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\n",
      "\n",
      "2022-04-08 15:45:21,294 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:60: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:73: The name tf.train.StepCounterHook is deprecated. Please use tf.estimator.StepCounterHook instead.\n",
      "\n",
      "2022-04-08 15:45:21,294 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:73: The name tf.train.StepCounterHook is deprecated. Please use tf.estimator.StepCounterHook instead.\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2022-04-08 15:45:21,294 [INFO] tensorflow: Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:99: The name tf.train.SummarySaverHook is deprecated. Please use tf.estimator.SummarySaverHook instead.\n",
      "\n",
      "2022-04-08 15:45:21,294 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:99: The name tf.train.SummarySaverHook is deprecated. Please use tf.estimator.SummarySaverHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "2022-04-08 15:45:21,294 [WARNING] tensorflow: From /opt/tlt/.cache/dazel/_dazel_tlt/75913d2aee35770fa76c4a63d877f3aa/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2022-04-08 15:45:21,954 [INFO] tensorflow: Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2022-04-08 15:45:23,348 [INFO] tensorflow: Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2022-04-08 15:45:23,888 [INFO] tensorflow: Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for step-0.\n",
      "2022-04-08 15:45:29,300 [INFO] tensorflow: Saving checkpoints for step-0.\n",
      "INFO:tensorflow:epoch = 0.0, learning_rate = 4.9999994e-06, loss = 0.033097062, step = 0\n",
      "2022-04-08 15:45:54,776 [INFO] tensorflow: epoch = 0.0, learning_rate = 4.9999994e-06, loss = 0.033097062, step = 0\n",
      "2022-04-08 15:45:54,778 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 0/2: loss: 0.03310 learning rate: 0.00000 Time taken: 0:00:00 ETA: 0:00:00\n",
      "2022-04-08 15:45:54,778 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 0.301\n",
      "INFO:tensorflow:global_step/sec: 1.02882\n",
      "2022-04-08 15:45:58,665 [INFO] tensorflow: global_step/sec: 1.02882\n",
      "INFO:tensorflow:epoch = 0.15555555555555556, learning_rate = 0.00017969075, loss = 0.03151534, step = 7 (5.644 sec)\n",
      "2022-04-08 15:46:00,420 [INFO] tensorflow: epoch = 0.15555555555555556, learning_rate = 0.00017969075, loss = 0.03151534, step = 7 (5.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71062\n",
      "2022-04-08 15:46:01,004 [INFO] tensorflow: global_step/sec: 1.71062\n",
      "INFO:tensorflow:global_step/sec: 1.60495\n",
      "2022-04-08 15:46:03,496 [INFO] tensorflow: global_step/sec: 1.60495\n",
      "INFO:tensorflow:global_step/sec: 1.67029\n",
      "2022-04-08 15:46:05,891 [INFO] tensorflow: global_step/sec: 1.67029\n",
      "INFO:tensorflow:epoch = 0.37777777777777777, learning_rate = 0.00049999997, loss = 0.022096299, step = 17 (6.059 sec)\n",
      "2022-04-08 15:46:06,479 [INFO] tensorflow: epoch = 0.37777777777777777, learning_rate = 0.00049999997, loss = 0.022096299, step = 17 (6.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.62152\n",
      "2022-04-08 15:46:08,358 [INFO] tensorflow: global_step/sec: 1.62152\n",
      "INFO:tensorflow:global_step/sec: 1.64723\n",
      "2022-04-08 15:46:10,786 [INFO] tensorflow: global_step/sec: 1.64723\n",
      "2022-04-08 15:46:10,787 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 3.414\n",
      "INFO:tensorflow:epoch = 0.6, learning_rate = 0.00049999997, loss = 0.015778359, step = 27 (6.118 sec)\n",
      "2022-04-08 15:46:12,597 [INFO] tensorflow: epoch = 0.6, learning_rate = 0.00049999997, loss = 0.015778359, step = 27 (6.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66722\n",
      "2022-04-08 15:46:13,185 [INFO] tensorflow: global_step/sec: 1.66722\n",
      "INFO:tensorflow:global_step/sec: 1.65538\n",
      "2022-04-08 15:46:15,601 [INFO] tensorflow: global_step/sec: 1.65538\n",
      "INFO:tensorflow:global_step/sec: 1.62554\n",
      "2022-04-08 15:46:18,062 [INFO] tensorflow: global_step/sec: 1.62554\n",
      "INFO:tensorflow:epoch = 0.8222222222222223, learning_rate = 0.00049999997, loss = 0.011484692, step = 37 (6.126 sec)\n",
      "2022-04-08 15:46:18,723 [INFO] tensorflow: epoch = 0.8222222222222223, learning_rate = 0.00049999997, loss = 0.011484692, step = 37 (6.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66042\n",
      "2022-04-08 15:46:20,471 [INFO] tensorflow: global_step/sec: 1.66042\n",
      "INFO:tensorflow:global_step/sec: 1.66186\n",
      "2022-04-08 15:46:22,878 [INFO] tensorflow: global_step/sec: 1.66186\n",
      "e934870610c5:46:72 [0] NCCL INFO Bootstrap : Using lo:127.0.0.1<0>\n",
      "e934870610c5:46:72 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "e934870610c5:46:72 [0] NCCL INFO NET/IB : No device found.\n",
      "e934870610c5:46:72 [0] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1<0> [1]eth0:172.17.0.3<0>\n",
      "e934870610c5:46:72 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.9.9+cuda11.3\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 00/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 01/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 02/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 03/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 04/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 05/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 06/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 07/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 08/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 09/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 10/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 11/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 12/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 13/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 14/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 15/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 16/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 17/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 18/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 19/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 20/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 21/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 22/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 23/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 24/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 25/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 26/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 27/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 28/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 29/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 30/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Channel 31/32 :    0\n",
      "e934870610c5:46:72 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "e934870610c5:46:72 [0] NCCL INFO Connected all rings\n",
      "e934870610c5:46:72 [0] NCCL INFO Connected all trees\n",
      "e934870610c5:46:72 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "e934870610c5:46:72 [0] NCCL INFO comm 0x7f27b43215a0 rank 0 nranks 1 cudaDev 0 busId 1000 - Init COMPLETE\n",
      "2022-04-08 15:46:23,140 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 11, 0.00s/step\n",
      "2022-04-08 15:46:27,239 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 11, 0.41s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 18203/18203 [00:01<00:00, 17164.29it/s]\n",
      "Epoch 1/2\n",
      "=========================\n",
      "\n",
      "Validation cost: 0.000623\n",
      "Mean average_precision (in %): 1.7747\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "lpd                              1.77473\n",
      "\n",
      "Median Inference Time: 0.049368\n",
      "INFO:tensorflow:epoch = 1.0, learning_rate = 0.00049999997, loss = 0.00020948345, step = 45 (10.857 sec)\n",
      "2022-04-08 15:46:29,580 [INFO] tensorflow: epoch = 1.0, learning_rate = 0.00049999997, loss = 0.00020948345, step = 45 (10.857 sec)\n",
      "2022-04-08 15:46:29,580 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 1/2: loss: 0.00021 learning rate: 0.00050 Time taken: 0:00:47.497598 ETA: 0:00:47.497598\n",
      "INFO:tensorflow:global_step/sec: 0.472871\n",
      "2022-04-08 15:46:31,337 [INFO] tensorflow: global_step/sec: 0.472871\n",
      "2022-04-08 15:46:31,920 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 4.732\n",
      "INFO:tensorflow:global_step/sec: 1.64484\n",
      "2022-04-08 15:46:33,769 [INFO] tensorflow: global_step/sec: 1.64484\n",
      "INFO:tensorflow:epoch = 1.2222222222222223, learning_rate = 0.00049999997, loss = 9.596761e-05, step = 55 (5.970 sec)\n",
      "2022-04-08 15:46:35,550 [INFO] tensorflow: epoch = 1.2222222222222223, learning_rate = 0.00049999997, loss = 9.596761e-05, step = 55 (5.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68929\n",
      "2022-04-08 15:46:36,137 [INFO] tensorflow: global_step/sec: 1.68929\n",
      "INFO:tensorflow:global_step/sec: 1.55566\n",
      "2022-04-08 15:46:38,708 [INFO] tensorflow: global_step/sec: 1.55566\n",
      "INFO:tensorflow:global_step/sec: 1.66778\n",
      "2022-04-08 15:46:41,106 [INFO] tensorflow: global_step/sec: 1.66778\n",
      "INFO:tensorflow:epoch = 1.4444444444444444, learning_rate = 0.00035548513, loss = 0.00028612188, step = 65 (6.140 sec)\n",
      "2022-04-08 15:46:41,690 [INFO] tensorflow: epoch = 1.4444444444444444, learning_rate = 0.00035548513, loss = 0.00028612188, step = 65 (6.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6275\n",
      "2022-04-08 15:46:43,564 [INFO] tensorflow: global_step/sec: 1.6275\n",
      "INFO:tensorflow:global_step/sec: 1.68186\n",
      "2022-04-08 15:46:45,943 [INFO] tensorflow: global_step/sec: 1.68186\n",
      "2022-04-08 15:46:47,142 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 6.570\n",
      "INFO:tensorflow:epoch = 1.6666666666666667, learning_rate = 6.4577405e-05, loss = 0.0001120736, step = 75 (6.118 sec)\n",
      "2022-04-08 15:46:47,808 [INFO] tensorflow: epoch = 1.6666666666666667, learning_rate = 6.4577405e-05, loss = 0.0001120736, step = 75 (6.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.63354\n",
      "2022-04-08 15:46:48,391 [INFO] tensorflow: global_step/sec: 1.63354\n",
      "INFO:tensorflow:global_step/sec: 1.70035\n",
      "2022-04-08 15:46:50,744 [INFO] tensorflow: global_step/sec: 1.70035\n",
      "INFO:tensorflow:global_step/sec: 1.70116\n",
      "2022-04-08 15:46:53,095 [INFO] tensorflow: global_step/sec: 1.70116\n",
      "INFO:tensorflow:epoch = 1.888888888888889, learning_rate = 1.1731138e-05, loss = 9.7665965e-05, step = 85 (5.946 sec)\n",
      "2022-04-08 15:46:53,754 [INFO] tensorflow: epoch = 1.888888888888889, learning_rate = 1.1731138e-05, loss = 9.7665965e-05, step = 85 (5.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.61195\n",
      "2022-04-08 15:46:55,576 [INFO] tensorflow: global_step/sec: 1.61195\n",
      "2022-04-08 15:46:56,168 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 11, 0.00s/step\n",
      "2022-04-08 15:47:01,097 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 11, 0.49s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 26867/26867 [00:01<00:00, 17599.48it/s]\n",
      "Epoch 2/2\n",
      "=========================\n",
      "\n",
      "Validation cost: 0.000172\n",
      "Mean average_precision (in %): 12.1924\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "lpd                              12.1924\n",
      "\n",
      "Median Inference Time: 0.049662\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-04-08 15:47:03,869 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-04-08 15:47:03,869 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "INFO:tensorflow:Saving checkpoints for step-90.\n",
      "2022-04-08 15:47:03,871 [INFO] tensorflow: Saving checkpoints for step-90.\n",
      "2022-04-08 15:47:06,622 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 6.570\n",
      "Time taken to run __main__:main: 0:01:58.528081.\n",
      "2022-04-08 10:47:08,334 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao detectnet_v2 train -e $INT_SPECS_DIR/SPECS_train.txt \\\n",
    "                        -r $INT_MAIN_DIR/experiments/dir_$MODEL_NAME \\\n",
    "                        -k nvidia_tlt \\\n",
    "                        -n $MODEL_NAME\n",
    "#\\\n",
    "                        #--log_file $INT_MAIN_DIR/experiments/dir_$MODEL_NAME/$LOG_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:8\", \"status\": \"Starting DetectNet_v2 Training job\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:8\", \"status\": \"Training gridbox model.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:8\", \"status\": \"Building DetectNet V2 model\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"DetectNet V2 model built.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"Building rasterizer.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"Rasterizers built.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"Building training graph.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"Rasterizing tensors.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:17\", \"status\": \"Tensors rasterized.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:19\", \"status\": \"Training graph built.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:19\", \"status\": \"Building validation graph.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:19\", \"status\": \"Rasterizing tensors.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:20\", \"status\": \"Tensors rasterized.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:20\", \"status\": \"Validation graph built.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:45:21\", \"status\": \"Running training loop.\"}\n",
      "{\"loss\": 0.03309706225991249, \"cur_epoch\": 0, \"max_epoch\": 2, \"time_per_epoch\": \"0:00:00\", \"ETA\": \"0:00:00\", \"learning_rate\": 4.999999418942025e-06, \"date\": \"4/8/2022\", \"time\": \"15:45:54\", \"status\": \"Running.\"}\n",
      "{\"validation cost\": 0.00062254, \"mean average precision\": 1.7747, \"average_precision\": {\"lpd\": 1.7747}, \"date\": \"4/8/2022\", \"time\": \"15:46:28\", \"status\": \"Evaluation Complete\"}\n",
      "{\"loss\": 0.00020948344899807125, \"cur_epoch\": 1, \"max_epoch\": 2, \"time_per_epoch\": \"0:00:47.497598\", \"ETA\": \"0:00:47.497598\", \"learning_rate\": 0.0004999999655410647, \"date\": \"4/8/2022\", \"time\": \"15:46:29\", \"status\": \"Running.\"}\n",
      "{\"validation cost\": 0.00017171, \"mean average precision\": 12.1924, \"average_precision\": {\"lpd\": 12.1924}, \"date\": \"4/8/2022\", \"time\": \"15:47:3\", \"status\": \"Evaluation Complete\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:47:3\", \"status\": \"Saving trained model.\"}\n",
      "{\"size\": 42.91735076904297, \"param_count\": 11.197893, \"date\": \"4/8/2022\", \"time\": \"15:47:3\", \"status\": \"Model saved.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:47:6\", \"status\": \"Training loop completed.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:47:6\", \"status\": \"Training op complete.\"}\n",
      "{\"date\": \"4/8/2022\", \"time\": \"15:47:6\", \"status\": \"DetectNet_v2 training job complete.\"}\n"
     ]
    }
   ],
   "source": [
    "!cat ./experiments/dir_lpd_net_test/status.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e $SPECS_DIR/detectnet_v2_train_resnet18_kitti.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "                        -k $KEY \\\n",
    "                        -n resnet18_detector \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for each epoch:\n",
      "---------------------\n",
      "total 0\n"
     ]
    }
   ],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -lh $LOCAL_PROJECT_DIR/experiments/experiment_test/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the trained model <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_train_resnet18_kitti.txt\\\n",
    "                           -m $INT_MAIN_DIR/experiment_dir_unpruned/weights/resnet18_detector.tlt \\\n",
    "                           -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prune the trained model <a class=\"anchor\" id=\"head-6\"></a>\n",
    "* Specify pre-trained model\n",
    "* Equalization criterion (`Applicable for resnets and mobilenets`)\n",
    "* Threshold for pruning.\n",
    "* A key to save and load the model\n",
    "* Output directory to store the model\n",
    "\n",
    "*Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is dependent on the dataset. A pth value `5.2e-6` is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy.*\n",
    "\n",
    "*For some internal studies, we have noticed that a pth value of 0.01 is a good starting point for detectnet_v2 models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist.\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 prune \\\n",
    "                  -m $INT_MAIN_DIR/experiment_dir_unpruned/weights/resnet18_detector.tlt \\\n",
    "                  -o $INT_MAIN_DIR/experiment_dir_pruned/resnet18_nopool_bn_detectnet_v2_pruned.tlt \\\n",
    "                  -eq union \\\n",
    "                  -pth 0.0000052 \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_pruned/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain the pruned model <a class=\"anchor\" id=\"head-7\"></a>\n",
    "* Model needs to be re-trained to bring back accuracy after pruning\n",
    "* Specify re-training specification with pretrained weights as pruned model.\n",
    "\n",
    "*Note: For retraining, please set the `load_graph` option to `true` in the model_config to load the pruned model graph. Also, if after retraining, the model shows some decrease in mAP, it could be that the originally trained model was pruned a little too much. Please try reducing the pruning threshold (thereby reducing the pruning ratio) and use the new model to retrain.*\n",
    "\n",
    "*Note: DetectNet_v2 now supports Quantization Aware Training, to help with optmizing the model. By default, the training in the cell below doesn't run the model with QAT enabled. For information on training a model with QAT, please refer to the cells under [section 11](#head-11)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to include the \n",
    "# newly pruned model as a pretrained weights and, the\n",
    "# load_graph option is set to true \n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_kitti.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tao detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti.txt \\\n",
    "                        -r $USER_EXPERIMENT_DIR/experiment_dir_retrain \\\n",
    "                        -k $KEY \\\n",
    "                        -n resnet18_detector_pruned \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the retrained model <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates the pruned and retrained model, using the `evaluate` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti.txt \\\n",
    "                           -m $INT_MAIN_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.tlt \\\n",
    "                           -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize inferences <a class=\"anchor\" id=\"head-9\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models. To render bboxes from more classes, please edit the spec file `detectnet_v2_inference_kitti_tlt.txt` to include all the classes you would like to visualize and edit the rest of the file accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_kitti_tlt.txt \\\n",
    "                            -o $USER_EXPERIMENT_DIR/tlt_infer_testing \\\n",
    "                            -i $INT_DATA_DIR/testing/image_2 \\\n",
    "                            -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference` tool produces two outputs. \n",
    "1. Overlain images in `$INT_MAIN_DIR/tlt_infer_testing/images_annotated`\n",
    "2. Frame by frame bbox labels in kitti format located in `$INT_MAIN_DIR/tlt_infer_testing/labels`\n",
    "\n",
    "*Note: To run inferences for a single image, simply replace the path to the -i flag in `inference` command with the path to the image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "!pip3 install matplotlib==3.3.3\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the first 12 images.\n",
    "OUTPUT_PATH = 'tlt_infer_testing/images_annotated' # relative path from $INT_MAIN_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Export <a class=\"anchor\" id=\"head-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_final\n",
    "# Removing a pre-existing copy of the etlt if there has been any.\n",
    "import os\n",
    "output_file=os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'],\n",
    "                         \"experiment_dir_final/resnet18_detector.etlt\")\n",
    "if os.path.exists(output_file):\n",
    "    os.system(\"rm {}\".format(output_file))\n",
    "!tao detectnet_v2 export \\\n",
    "                  -m $USER_EXPERIMENT_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.tlt \\\n",
    "                  -o $INT_MAIN_DIR/experiment_dir_final/resnet18_detector.etlt \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/experiment_dir_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Int8 Optimization <a class=\"anchor\" id=\"head-10-1\"></a>\n",
    "DetectNet_v2 model supports int8 inference mode in TensorRT. \n",
    "In order to use int8 mode, we must calibrate the model to run 8-bit inferences -\n",
    "\n",
    "* Generate calibration tensorfile from the training data using detectnet_v2 calibration_tensorfile\n",
    "* Use tao <task> export to generate int8 calibration table.\n",
    "\n",
    "*Note: For this example, we generate a calibration tensorfile containing 10 batches of training data.\n",
    "Ideally, it is best to use at least 10-20% of the training data to do so. The more data provided during calibration, the closer int8 inferences are to fp32 inferences.*\n",
    "\n",
    "*Note: If the model was trained with QAT nodes available, please refrain from using the post training int8 optimization as mentioned below. Please export the model in int8 mode (using the arg `--data_type int8`) with just the path to the calibration cache file (using the argument `--cal_cache_file`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 calibration_tensorfile -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti.txt \\\n",
    "                                         -m 10 \\\n",
    "                                         -o $INT_MAIN_DIR/experiment_dir_final/calibration.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.etlt\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/calibration.bin\n",
    "!tao detectnet_v2 export \\\n",
    "                  -m $INT_MAIN_DIR/experiment_dir_retrain/weights/resnet18_detector_pruned.tlt \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.etlt \\\n",
    "                  -k $KEY  \\\n",
    "                  --cal_data_file $INT_MAIN_DIR/experiment_dir_final/calibration.tensor \\\n",
    "                  --data_type int8 \\\n",
    "                  --batches 10 \\\n",
    "                  --batch_size 4 \\\n",
    "                  --max_batch_size 4\\\n",
    "                  --engine_file $INT_MAIN_DIR/experiment_dir_final/resnet18_detector.trt.int8 \\\n",
    "                  --cal_cache_file $INT_MAIN_DIR/experiment_dir_final/calibration.bin \\\n",
    "                  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate TensorRT engine <a class=\"anchor\" id=\"head-10-2\"></a>\n",
    "Verify engine generation using the `tao-converter` utility included with the docker.\n",
    "\n",
    "The `tao-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tao-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The tao-converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the tao-converter for jetson from the dev zone link [here](https://developer.nvidia.com/tao-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, you may do so by simply copying the exported `.etlt` file along with the calibration cache to the target device and updating the spec file that configures the `gst-nvinfer` element to point to this newly exported model. Usually this file is called `config_infer_primary.txt` for detection models and `config_infer_secondary_*.txt` for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao converter $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.etlt \\\n",
    "                   -k $KEY \\\n",
    "                    -c $INT_MAIN_DIR/experiment_dir_final/calibration.bin \\\n",
    "                   -o output_cov/Sigmoid,output_bbox/BiasAdd \\\n",
    "                   -d 3,384,1248 \\\n",
    "                   -i nchw \\\n",
    "                   -m 64 \\\n",
    "                   -t int8 \\\n",
    "                   -e $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector.trt \\\n",
    "                   -b 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Deployed Model <a class=\"anchor\" id=\"head-11\"></a>\n",
    "Verify the exported model by visualizing inferences on TensorRT.\n",
    "In addition to running inference on a `.tlt` model in [step 9](#head-9), the `inference` tool is also capable of consuming the converted `TensorRT engine` from [step 10.B](#head-10-2).\n",
    "\n",
    "*If after int-8 calibration the accuracy of the int-8 inferences seem to degrade, it could be because the there wasn't enough data in the calibration tensorfile used to calibrate thee model or, the training data is not entirely representative of your test images, and the calibration maybe incorrect. Therefore, you may either regenerate the calibration tensorfile with more batches of the training data and recalibrate the model, or calibrate the model on a few images from the test set. This may be done using `--cal_image_dir` flag in the `export` tool. For more information, please follow the instructions in the USER GUIDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Inference using TensorRT engine <a class=\"anchor\" id=\"head-11-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_kitti_etlt.txt \\\n",
    "                            -o $INT_MAIN_DIR/etlt_infer_testing \\\n",
    "                            -i $INT_DATA_DIR/testing/image_2 \\\n",
    "                            -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 12 inferenced images.\n",
    "OUTPUT_PATH = 'etlt_infer_testing/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. QAT workflow <a class=\"anchor\" id=\"head-12\"></a>\n",
    "This section delves into the newly enabled Quantization Aware Training feature with DetectNet_v2. The workflow defined below converts a pruned model from section [5](#head-5) to enable QAT and retrain this model to while accounting the noise introduced due to quantization in the forward pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Convert pruned model to QAT and retrain <a class=\"anchor\" id=\"head-12-1\"></a>\n",
    "All detectnet models, unpruned and pruned models can be converted to QAT models by setting the `enable_qat` parameter in the `training_config` component of the spec file to `true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the retrain experiment file. \n",
    "# Note: We have updated the experiment file to convert the\n",
    "# pretrained model to qat mode by setting the enable_qat\n",
    "# parameter.\n",
    "!cat $LOCAL_SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 train -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                        -r $INT_MAIN_DIR/experiment_dir_retrain_qat \\\n",
    "                        -k $KEY \\\n",
    "                        -n resnet18_detector_pruned_qat \\\n",
    "                        --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $LOCAL_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Evaluate QAT converted model <a class=\"anchor\" id=\"head-12-2\"></a>\n",
    "This section evaluates a QAT enabled pruned retrained model. The mAP of this model should be comparable to that of the pruned retrained model without QAT. However, due to quantization, it is possible sometimes to see a drop in the mAP value for certain datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                           -m $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.tlt \\\n",
    "                           -k $KEY \\\n",
    "                           -f tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Export QAT trained model to int8 <a class=\"anchor\" id=\"head-12-3\"></a>\n",
    "Export a QAT trained model to TensorRT parsable model. This command generates an .etlt file from the trained model and the serializes corresponding int8 scales as a TRT readable calibration cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.etlt\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/experiment_dir_final/calibration_qat.bin\n",
    "!tao detectnet_v2 export \\\n",
    "                  -m $INT_MAIN_DIR/experiment_dir_retrain_qat/weights/resnet18_detector_pruned_qat.tlt \\\n",
    "                  -o $USER_EXPERIMENT_DIR/experiment_dir_final/resnet18_detector_qat.etlt \\\n",
    "                  -k $KEY  \\\n",
    "                  --data_type int8 \\\n",
    "                  --batch_size 64 \\\n",
    "                  --max_batch_size 64\\\n",
    "                  --engine_file $INT_MAIN_DIR/experiment_dir_final/resnet18_detector_qat.trt.int8 \\\n",
    "                  --cal_cache_file $USER_EXPERIMENT_DIR/experiment_dir_final/calibration_qat.bin \\\n",
    "                  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Evaluate a QAT trained model using the exported TensorRT engine <a class=\"anchor\" id=\"head-12-4\"></a>\n",
    "This section evaluates a QAT enabled pruned retrained model using the TensorRT int8 engine that was exported in [Section C](#head-12-3). Please note that there maybe a slight difference (~0.1-0.5%) in the mAP from [Section B](#head-12-2), oweing to some differences in the implementation of quantization in TensorRT.\n",
    "\n",
    "*Note: The TensorRT evaluator might be slightly slower than the TAO evaluator here, because the evaluation dataloader is pinned to the CPU to avoid any clashes between TensorRT and TAO instances in the GPU. Please note that this tool was not intended and has not been developed for profiling the model. It is just a means to qualitatively analyse the model.*\n",
    "\n",
    "*Please use native TensorRT or DeepStream for the most optimized inferences.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 evaluate -e $SPECS_DIR/detectnet_v2_retrain_resnet18_kitti_qat.txt \\\n",
    "                           -m $INT_MAIN_DIR/experiment_dir_final/resnet18_detector_qat.trt.int8 \\\n",
    "                           -f tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Inference using QAT engine <a class=\"anchor\" id=\"head-12-5\"></a>\n",
    "Run inference and visualize detections on test images, using the exported TensorRT engine from [Section C](#head-12-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao detectnet_v2 inference -e $SPECS_DIR/detectnet_v2_inference_kitti_etlt_qat.txt \\\n",
    "                            -o $USER_EXPERIMENT_DIR/tlt_infer_testing_qat \\\n",
    "                            -i $INT_DATA_DIR/testing/image_2 \\\n",
    "                            -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 12 inferenced images.\n",
    "OUTPUT_PATH = 'tlt_infer_testing_qat/images_annotated' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 4 # number of columns in the visualizer grid.\n",
    "IMAGES = 12 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7bf828941e6e4480eb27ae3ebfd5c71ef231262edc548bb94b62ed7c15fcdaa"
  },
  "kernelspec": {
   "display_name": "mobia",
   "language": "python",
   "name": "mobia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
