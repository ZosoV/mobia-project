{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License Plate Recognition using TAO LPRNet\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained baseline18 LPRNet model and train it on the OpenALPR benchmark dataset\n",
    "* Run Inference on the trained model\n",
    "* Export the trained model to a .etlt file for deployment to DeepStream\n",
    "* Run inference on the exported. etlt model to verify deployment using TensorRT\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of LPRNet using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "0. [Set up env variables and map drives](#head-0)\n",
    "1. [Installing the TAO launcher](#head-1)\n",
    "2. [Prepare dataset and pre-trained model](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate trained models](#head-5)\n",
    "6. [Inferences](#head-6)\n",
    "7. [Deploy](#head-7)\n",
    "8. [Verify the deployed model](#head-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up env variables and map drives <a class=\"anchor\" id=\"head-0\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/lprnet`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n",
    "\n",
    "*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please replace the variable with your key.\n",
      "env: KEY=nvidia_tlt\n",
      "env: GPU_INDEX=0\n",
      "\n",
      "Internal Paths for Tao Workspace:\n",
      "\n",
      "env: INT_MAIN_DIR=/workspace/tao-experiments/lprnet\n",
      "env: INT_DATA_DIR=/workspace/tao-experiments/lprnet/data\n",
      "env: INT_SPECS_DIR=/workspace/tao-experiments/lprnet/specs\n",
      "\n",
      "Paths for local machine:\n",
      "\n",
      "LOCAL_PROJECT_DIR: /home/pepeleduin/mobia/mobia-project/training/lprnet\n",
      "LOCAL_DATA_DIR: /home/pepeleduin/mobia/mobia-project/training/lprnet/data\n",
      "LOCAL_EXPERIMENT_DIR: /home/pepeleduin/mobia/mobia-project/training/lprnet/experiments\n",
      "LOCAL_SPECS_DIR: /home/pepeleduin/mobia/mobia-project/training/lprnet/specs\n"
     ]
    }
   ],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "print(\"Please replace the variable with your key.\")\n",
    "%env KEY=nvidia_tlt\n",
    "%env GPU_INDEX=0\n",
    "!printf \"\\nInternal Paths for Tao Workspace:\\n\\n\" \n",
    "%env INT_MAIN_DIR=/workspace/tao-experiments/lprnet\n",
    "%env INT_DATA_DIR=/workspace/tao-experiments/lprnet/data\n",
    "%env INT_SPECS_DIR=/workspace/tao-experiments/lprnet/specs\n",
    "\n",
    "# Set this path if you don't run the notebook from the samples directory.\n",
    "# %env NOTEBOOK_ROOT=~/tao-samples/detectnet_v2\n",
    "\n",
    "# Please define this local project directory that needs to be mapped to the TAO docker session.\n",
    "# The dataset expected to be present in $LOCAL_PROJECT_DIR/data, while the results for the steps\n",
    "# in this notebook will be stored at $LOCAL_PROJECT_DIR/lprnet\n",
    "os.environ[\"LOCAL_PROJECT_DIR\"] = os.getcwd()\n",
    "\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"] = os.path.join(\n",
    "    os.getenv(\"LOCAL_PROJECT_DIR\", os.getcwd()),\n",
    "    \"experiments\"\n",
    ")\n",
    "\n",
    "# The sample spec files are present in the same path as the downloaded samples.\n",
    "os.environ[\"LOCAL_SPECS_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"specs\"\n",
    ")\n",
    "\n",
    "os.environ[\"LOCAL_DATA_DIR\"] = os.path.join(\n",
    "    os.getenv(\"NOTEBOOK_ROOT\", os.getcwd()),\n",
    "    \"data\"\n",
    ")\n",
    "\n",
    "# Showing list of specification files.\n",
    "# !ls -rlt $LOCAL_SPECS_DIR\n",
    "!printf \"\\nPaths for local machine:\\n\\n\" \n",
    "!echo LOCAL_PROJECT_DIR:    $LOCAL_PROJECT_DIR\n",
    "!echo LOCAL_DATA_DIR:       $LOCAL_DATA_DIR\n",
    "!echo LOCAL_EXPERIMENT_DIR: $LOCAL_EXPERIMENT_DIR\n",
    "!echo LOCAL_SPECS_DIR:      $LOCAL_SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. For more information please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide.\n",
    "\n",
    "When running this cell on AWS, update the drive_map entry with the dictionary defined below, so that you don't have permission issues when writing data into folders created by the TAO docker.\n",
    "\n",
    "```json\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "            # Mapping the data directory\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "                \"destination\": \"/workspace/tao-experiments\"\n",
    "            },\n",
    "            # Mapping the specs directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "                \"destination\": os.environ[\"SPECS_DIR\"]\n",
    "            },\n",
    "        ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping up the local directories to the TAO docker.\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "# Define the dictionary with the mapped drives\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "        # Mapping the data directory\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "            \"destination\": os.environ[\"INT_MAIN_DIR\"]\n",
    "        },\n",
    "        # Mapping the specs directory.\n",
    "        {\n",
    "            \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "            \"destination\": os.environ[\"INT_SPECS_DIR\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Writing the mounts file.\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Mounts\": [\n",
      "        {\n",
      "            \"source\": \"/home/pepeleduin/mobia/mobia-project/training/lprnet\",\n",
      "            \"destination\": \"/workspace/tao-experiments/lprnet\"\n",
      "        },\n",
      "        {\n",
      "            \"source\": \"/home/pepeleduin/mobia/mobia-project/training/lprnet/specs\",\n",
      "            \"destination\": \"/workspace/tao-experiments/lprnet/specs\"\n",
      "        }\n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (21.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 679 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (58.0.4)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-62.0.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (0.37.1)\n",
      "Installing collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n",
      "      Successfully uninstalled setuptools-58.0.4\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.2\n",
      "    Uninstalling pip-21.2.2:\n",
      "      Successfully uninstalled pip-21.2.2\n",
      "Successfully installed pip-22.0.4 setuptools-62.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy==1.17.0 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: opencv-python in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 2)) (4.5.5.64)\n",
      "Requirement already satisfied: pillow==8.1.0 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 3)) (8.1.0)\n",
      "Requirement already satisfied: matplotlib==3.1.1 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: scipy==1.5.4 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: h5py==3.1.0 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: joblib==1.0.1 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: pycocotools==2.0.2 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from -r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 8)) (2.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: cached-property in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from h5py==3.1.0->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from pycocotools==2.0.2->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 8)) (0.29.28)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from pycocotools==2.0.2->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 8)) (62.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/pepeleduin/anaconda3/envs/mobia_lpr/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1->-r /home/pepeleduin/mobia/mobia-project/training/lprnet/../deps/requirements-pip.txt (line 4)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the dependencies\n",
    "import os\n",
    "os.environ[\"PROJECT_DIR\"] = os.getcwd()\n",
    "!pip3 install -r $PROJECT_DIR/../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the TAO launcher <a class=\"anchor\" id=\"head-1\"></a>\n",
    "The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index. You may install the launcher by executing the following cell.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python 3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the `virtualenv` and `virtualenvwrapper` packages. Once you have setup virtualenvwrapper, please set the version of python to be used in the virtual env by using the `VIRTUALENVWRAPPER_PYTHON` variable. You may do so by running\n",
    "\n",
    "```sh\n",
    "export VIRTUALENVWRAPPER_PYTHON=/path/to/bin/python3.x\n",
    "```\n",
    "where x >= 6 and <= 8\n",
    "\n",
    "We recommend performing this step first and then launching the notebook from the virtual environment. In addition to installing TAO python package, please make sure of the following software requirements:\n",
    "* python >=3.6.9 < 3.8.x\n",
    "* docker-ce > 19.03.5\n",
    "* docker-API 1.40\n",
    "* nvidia-container-toolkit > 1.3.0-1\n",
    "* nvidia-container-runtime > 3.4.0-1\n",
    "* nvidia-docker2 > 2.5.0-1\n",
    "* nvidia-driver > 455+\n",
    "\n",
    "Once you have installed the pre-requisites, please log in to the docker registry nvcr.io by following the command below\n",
    "\n",
    "```sh\n",
    "docker login nvcr.io\n",
    "```\n",
    "\n",
    "You will be triggered to enter a username and password. The username is `$oauthtoken` and the password is the API key generated from `ngc.nvidia.com`. Please follow the instructions in the [NGC setup guide](https://docs.nvidia.com/ngc/ngc-overview/index.html#generating-api-key) to generate your own API key.\n",
    "\n",
    "Please note that TAO Toolkit recommends users to run the TAO launcher in a virtual env with python >=3.6.9. You may follow the instruction in this [page](https://virtualenvwrapper.readthedocs.io/en/latest/install.html) to set up a python virtual env using the virtualenv and virtualenvwrapper packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8415 sha256=dd065ca3a3021992b42e3ae4ddd9c3229d35dd2e6e9be99072f6bfe15bbf1a29\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ihpjcagm/wheels/f1/a1/a1/6cc45cc1ae6b1876f12ef399c0d0d6e18809e9ced611c7c2a7\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvidia-tao\n",
      "  Downloading nvidia_tao-0.1.21-py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 KB\u001b[0m \u001b[31m881.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds==0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting docker==4.3.1\n",
      "  Downloading docker-4.3.1-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/145.2 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi==2020.6.20\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna==2.10\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tabulate==0.8.7\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting requests==2.24.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chardet==3.0.4\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting six==1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting websocket-client==0.57.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3==1.25.10\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.6/127.6 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tabulate, chardet, certifi, urllib3, six, idna, websocket-client, requests, docker-pycreds, docker, nvidia-tao\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2021.10.8\n",
      "    Uninstalling certifi-2021.10.8:\n",
      "      Successfully uninstalled certifi-2021.10.8\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 docker-4.3.1 docker-pycreds-0.4.0 idna-2.10 nvidia-tao-0.1.21 requests-2.24.0 six-1.15.0 tabulate-0.8.7 urllib3-1.25.10 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "# SKIP this step IF you have already installed the TAO launcher.\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of the TAO Toolkit Instance\n",
      "dockers: ['nvidia/tao/tao-toolkit-tf', 'nvidia/tao/tao-toolkit-pyt', 'nvidia/tao/tao-toolkit-lm']\n",
      "format_version: 2.0\n",
      "toolkit_version: 3.22.02\n",
      "published_date: 02/28/2022\n"
     ]
    }
   ],
   "source": [
    "# View the versions of the TAO launcher\n",
    "!tao info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the OpenALPR benchmark dataset for the tutorial. The following script will download the dataset automatically and convert it to the format used by TAO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pepeleduin/mobia/mobia-project/training/lprnet/data\n"
     ]
    }
   ],
   "source": [
    "!echo $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $LOCAL_DATA_DIR\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ '[' -z /home/pepeleduin/mobia/mobia-project/training/lprnet/data ']'\n",
      "++ pwd\n",
      "+ CURRENT_DIR=/home/pepeleduin/mobia/mobia-project/training/lprnet\n",
      "+ echo 'Cloning OpenALPR benchmark directory'\n",
      "Cloning OpenALPR benchmark directory\n",
      "+ '[' '!' -e benchmarks ']'\n",
      "+ OUTPUT_DIR=/home/pepeleduin/mobia/mobia-project/training/lprnet/data\n",
      "+ mkdir -p /home/pepeleduin/mobia/mobia-project/training/lprnet/data\n",
      "+++ readlink -f download_and_prepare_data.sh\n",
      "++ dirname /home/pepeleduin/mobia/mobia-project/training/lprnet/download_and_prepare_data.sh\n",
      "+ SCRIPT_DIR=/home/pepeleduin/mobia/mobia-project/training/lprnet\n",
      "+ echo 'Preprocessing OpenALPR benchmarks data for US'\n",
      "Preprocessing OpenALPR benchmarks data for US\n",
      "+ python3 /home/pepeleduin/mobia/mobia-project/training/lprnet/preprocess_openalpr_benchmark.py --input_dir=/home/pepeleduin/mobia/mobia-project/training/lprnet/benchmarks/endtoend/us/ --output_dir=/home/pepeleduin/mobia/mobia-project/training/lprnet/data\n",
      "Total 222 samples in benchmark dataset\n",
      "111 for train and 111 for val\n"
     ]
    }
   ],
   "source": [
    "!bash download_and_prepare_data.sh $LOCAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x 4 pepeleduin pepeleduin 4096 Apr  8 10:27 train\n",
      "drwxr-xr-x 4 pepeleduin pepeleduin 4096 Apr  8 10:27 val\n",
      "total 8\n",
      "drwxr-xr-x 2 pepeleduin pepeleduin 4096 Apr  8 10:27 image\n",
      "drwxr-xr-x 2 pepeleduin pepeleduin 4096 Apr  8 10:27 label\n",
      "total 680\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1882 Apr  8 12:23 1e241dc8-8f18-4955-8988-03a0ab49f813.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1925 Apr  8 12:23 21d8c31d-3deb-494b-9c63-c0223306fd82.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 10710 Apr  8 12:23 316b64c0-55bf-4079-a1c0-d93f461a576f.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2003 Apr  8 12:23 33fa5185-0286-4e8f-b775-46162eba39d4.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2092 Apr  8 12:23 37170dd1-2802-4e38-b982-c5d07c64ff67.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2067 Apr  8 12:23 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1860 Apr  8 12:23 5b562a61-34ad-4f00-9164-d34abb7a38e4.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2096 Apr  8 12:23 c9368c55-210d-456c-a5ef-c310e60039ec.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 14111 Apr  8 12:23 car13.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3431 Apr  8 12:23 car14.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1748 Apr  8 12:23 car17.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2387 Apr  8 12:23 car18.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 25056 Apr  8 12:23 car19.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3355 Apr  8 12:23 car2.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  5366 Apr  8 12:23 car3.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 11377 Apr  8 12:23 car7.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2537 Apr  8 12:23 car8.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6907 Apr  8 12:23 car9-0.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6260 Apr  8 12:23 car9-7.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3626 Apr  8 12:23 car9-9.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3125 Apr  8 12:23 car9.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1974 Apr  8 12:23 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  5125 Apr  8 12:23 us1.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4663 Apr  8 12:23 us2.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  9130 Apr  8 12:23 us4.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 13834 Apr  8 12:23 us5.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 14499 Apr  8 12:23 us6.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 38963 Apr  8 12:23 us8.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2865 Apr  8 12:23 wts-lg-000013.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1703 Apr  8 12:23 wts-lg-000018.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  5038 Apr  8 12:23 wts-lg-000020.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2397 Apr  8 12:23 wts-lg-000022.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2762 Apr  8 12:23 wts-lg-000023.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3010 Apr  8 12:23 wts-lg-000025.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6964 Apr  8 12:23 wts-lg-000027.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2506 Apr  8 12:23 wts-lg-000029.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4293 Apr  8 12:23 wts-lg-000031.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3035 Apr  8 12:23 wts-lg-000032.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2198 Apr  8 12:23 wts-lg-000035.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2849 Apr  8 12:23 wts-lg-000038.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2160 Apr  8 12:23 wts-lg-000039.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3907 Apr  8 12:23 wts-lg-000041.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3343 Apr  8 12:23 wts-lg-000043.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2866 Apr  8 12:23 wts-lg-000044.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2045 Apr  8 12:23 wts-lg-000046.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1862 Apr  8 12:23 wts-lg-000047.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2896 Apr  8 12:23 wts-lg-000049.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2773 Apr  8 12:23 wts-lg-000052.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2040 Apr  8 12:23 wts-lg-000053.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1929 Apr  8 12:23 wts-lg-000055.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3636 Apr  8 12:23 wts-lg-000057.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3184 Apr  8 12:23 wts-lg-000062.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2069 Apr  8 12:23 wts-lg-000067.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4094 Apr  8 12:23 wts-lg-000070.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2084 Apr  8 12:23 wts-lg-000072.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3319 Apr  8 12:23 wts-lg-000073.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  5239 Apr  8 12:23 wts-lg-000077.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3268 Apr  8 12:23 wts-lg-000078.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3139 Apr  8 12:23 wts-lg-000082.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3542 Apr  8 12:23 wts-lg-000084.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3043 Apr  8 12:23 wts-lg-000085.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  1887 Apr  8 12:23 wts-lg-000088.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2293 Apr  8 12:23 wts-lg-000091.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2953 Apr  8 12:23 wts-lg-000096.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3838 Apr  8 12:23 wts-lg-000098.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3725 Apr  8 12:23 wts-lg-000099.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3232 Apr  8 12:23 wts-lg-000100.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3084 Apr  8 12:23 wts-lg-000101.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2654 Apr  8 12:23 wts-lg-000102.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2304 Apr  8 12:23 wts-lg-000113.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3483 Apr  8 12:23 wts-lg-000115.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2675 Apr  8 12:23 wts-lg-000118.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3442 Apr  8 12:23 wts-lg-000120.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2286 Apr  8 12:23 wts-lg-000121.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2021 Apr  8 12:23 wts-lg-000122.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3239 Apr  8 12:23 wts-lg-000127.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3027 Apr  8 12:23 wts-lg-000128.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6309 Apr  8 12:23 wts-lg-000129.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3119 Apr  8 12:23 wts-lg-000131.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3492 Apr  8 12:23 wts-lg-000133.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6167 Apr  8 12:23 wts-lg-000135.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3228 Apr  8 12:23 wts-lg-000140.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2880 Apr  8 12:23 wts-lg-000141.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  7493 Apr  8 12:23 wts-lg-000144.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3232 Apr  8 12:23 wts-lg-000145.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2734 Apr  8 12:23 wts-lg-000146.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4260 Apr  8 12:23 wts-lg-000150.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3020 Apr  8 12:23 wts-lg-000151.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6083 Apr  8 12:23 wts-lg-000155.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2580 Apr  8 12:23 wts-lg-000156.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4291 Apr  8 12:23 wts-lg-000157.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  5061 Apr  8 12:23 wts-lg-000159.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  7101 Apr  8 12:23 wts-lg-000162.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  7254 Apr  8 12:23 wts-lg-000164.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  7106 Apr  8 12:23 wts-lg-000165.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3097 Apr  8 12:23 wts-lg-000166.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4216 Apr  8 12:23 wts-lg-000167.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  8129 Apr  8 12:23 wts-lg-000168.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4709 Apr  8 12:23 wts-lg-000169.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4325 Apr  8 12:23 wts-lg-000170.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  6751 Apr  8 12:23 wts-lg-000171.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3706 Apr  8 12:23 wts-lg-000173.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4448 Apr  8 12:23 wts-lg-000174.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  7503 Apr  8 12:23 wts-lg-000175.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  8097 Apr  8 12:23 wts-lg-000176.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4624 Apr  8 12:23 wts-lg-000177.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2049 Apr  8 12:23 wts-lg-000189.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  4504 Apr  8 12:23 wts-lg-000190.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3808 Apr  8 12:23 wts-lg-000192.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  3259 Apr  8 12:23 wts-lg-000194.jpg\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin  2762 Apr  8 12:23 wts-lg-000199.jpg\n",
      "total 444\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 1e241dc8-8f18-4955-8988-03a0ab49f813.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 21d8c31d-3deb-494b-9c63-c0223306fd82.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 316b64c0-55bf-4079-a1c0-d93f461a576f.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 33fa5185-0286-4e8f-b775-46162eba39d4.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 37170dd1-2802-4e38-b982-c5d07c64ff67.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 5b562a61-34ad-4f00-9164-d34abb7a38e4.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 c9368c55-210d-456c-a5ef-c310e60039ec.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car13.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car14.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car17.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car18.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car19.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car2.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car3.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 5 Apr  8 12:23 car7.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car8.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car9-0.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car9-7.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 car9-9.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 car9.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 us1.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 us2.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 us4.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 8 Apr  8 12:23 us5.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 us6.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 us8.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000013.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000018.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000020.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000022.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000023.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000025.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000027.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000029.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000031.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000032.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000035.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 5 Apr  8 12:23 wts-lg-000038.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000039.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000041.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000043.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000044.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000046.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000047.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000049.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000052.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000053.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000055.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 5 Apr  8 12:23 wts-lg-000057.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000062.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000067.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000070.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000072.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000073.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000077.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000078.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000082.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000084.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000085.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000088.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000091.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000096.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000098.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000099.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000100.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000101.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000102.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000113.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000115.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 5 Apr  8 12:23 wts-lg-000118.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000120.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000121.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000122.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000127.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000128.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000129.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000131.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000133.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000135.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000140.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000141.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000144.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000145.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000146.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000150.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000151.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000155.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000156.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000157.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000159.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000162.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000164.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000165.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000166.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000167.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000168.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000169.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000170.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000171.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000173.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000174.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000175.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000176.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000177.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 6 Apr  8 12:23 wts-lg-000189.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000190.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000192.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000194.txt\n",
      "-rw-r--r-- 1 pepeleduin pepeleduin 7 Apr  8 12:23 wts-lg-000199.txt\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!ls -l $LOCAL_DATA_DIR/\n",
    "!ls -l $LOCAL_DATA_DIR/train\n",
    "!ls -l $LOCAL_DATA_DIR/train/image\n",
    "!ls -l $LOCAL_DATA_DIR/train/label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download pretrained model from NGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NGC CLI to get the pre-trained models. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2022-04-08 12:25:08--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.64.174.116, 18.64.174.58, 18.64.174.47, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.64.174.116|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32588132 (31M) [application/zip]\n",
      "Saving to: ‘/home/pepeleduin/mobia/mobia-project/training/lprnet/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  31.08M  4.17MB/s    in 7.7s    \n",
      "\n",
      "2022-04-08 12:25:18 (4.02 MB/s) - ‘/home/pepeleduin/mobia/mobia-project/training/lprnet/ngccli/ngccli_cat_linux.zip’ saved [32588132/32588132]\n",
      "\n",
      "Archive:  /home/pepeleduin/mobia/mobia-project/training/lprnet/ngccli/ngccli_cat_linux.zip\n",
      "  inflating: /home/pepeleduin/mobia/mobia-project/training/lprnet/ngccli/ngc  \n",
      " extracting: /home/pepeleduin/mobia/mobia-project/training/lprnet/ngccli/ngc.md5  \n"
     ]
    }
   ],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"),\n",
    "                                         os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| Versi | Accur | Epoch | Batch | GPU   | Memor | File  | Statu | Creat |\n",
      "| on    | acy   | s     | Size  | Model | y Foo | Size  | s     | ed    |\n",
      "|       |       |       |       |       | tprin |       |       | Date  |\n",
      "|       |       |       |       |       | t     |       |       |       |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| train | 99.67 | 120   | 1     | V100  | 221.1 | 221.0 | UPLOA | Aug   |\n",
      "| able_ |       |       |       |       |       | 6 MB  | D_COM | 24,   |\n",
      "| v1.0  |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| deplo | 99.67 | 120   | 1     | V100  | 110.1 | 110.0 | UPLOA | Aug   |\n",
      "| yable |       |       |       |       |       | 9 MB  | D_COM | 24,   |\n",
      "| _v1.0 |       |       |       |       |       |       | PLETE | 2021  |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "!ngc registry model list nvidia/tao/lprnet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_lprnet_baseline18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 221.13 MB in 1m 13s, Download speed: 3.03 MB/s               \n",
      "----------------------------------------------------\n",
      "Transfer id: lprnet_vtrainable_v1.0 Download status: Completed.\n",
      "Downloaded local path: /home/pepeleduin/mobia/mobia-project/training/lprnet/experiments/pretrained_lprnet_baseline18/lprnet_vtrainable_v1-1.0\n",
      "Total files downloaded: 4 \n",
      "Total downloaded size: 221.13 MB\n",
      "Started at: 2022-04-08 12:26:39.945037\n",
      "Completed at: 2022-04-08 12:27:53.020607\n",
      "Duration taken: 1m 13s\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/lprnet:trainable_v1.0 \\\n",
    "     --dest $LOCAL_EXPERIMENT_DIR/pretrained_lprnet_baseline18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 226376\n",
      "-rw------- 1 pepeleduin pepeleduin       200 Apr  8 11:14 ch_lp_characters.txt\n",
      "-rw------- 1 pepeleduin pepeleduin 115963904 Apr  8 11:14 ch_lprnet_baseline18_trainable.tlt\n",
      "-rw------- 1 pepeleduin pepeleduin        70 Apr  8 11:14 us_lp_characters.txt\n",
      "-rw------- 1 pepeleduin pepeleduin 115832832 Apr  8 11:14 us_lprnet_baseline18_trainable.tlt\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provide training specification <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "* Note the spec $SPEC_DIR/default_sepc.txt is for training on US license plates:\n",
    "    * the max license plate length is 8;\n",
    "        * You can change `max_label_length` in `lpr_config` to satisfy your own dataset.\n",
    "    * the characters of US license plates are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, P, Q, R, S, T, U, V, W, X, Y, Z \n",
    "        * You can change `characters_list_file` in `dataset_config` to set your own characters.\n",
    "        * `characters_list_file` should contain all the characters in dataset. And one character takes one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/tutorial_spec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $LOCAL_SPECS_DIR/us_lp_characters.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run TAO training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For multi-GPU, change --gpus based on your machine.\n",
      "2022-04-08 12:28:06,357 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-04-08 12:28:06,421 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.21.11-tf1.15.5-py3\n",
      "2022-04-08 12:28:06,462 [WARNING] tlt.components.docker_handler.docker_handler: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/pepeleduin/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2022-04-08 17:28:11,992 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:57: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:60: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-04-08 17:28:11,992 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:60: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:61: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "2022-04-08 17:28:12,310 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/lprnet/scripts/train.py:61: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "2022-04-08 17:28:12,310 [INFO] iva.lprnet.utils.spec_loader: Merging specification from /workspace/tao-experiments/lprnet/specs/tutorial_spec.txt\n",
      "2022-04-08 17:28:12,315 [INFO] __main__: Loading pretrained weights. This may take a while...\n",
      "Initialize optimizer\n",
      "Model: \"lpnet_baseline_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_input (InputLayer)        [(None, 3, 48, 96)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 1, 48, 96)]  0           image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 48, 96)   640         tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 48, 96)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 64, 48, 96)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 48, 96)   0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 64, 48, 96)   36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 64, 48, 96)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 64, 48, 96)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 64, 48, 96)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 64, 48, 96)   36928       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 64, 48, 96)   256         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 64, 48, 96)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(None, 64, 48, 96)] 0           bn2a_branch1[0][0]               \n",
      "                                                                 bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 64, 48, 96)   0           tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 64, 48, 96)   36928       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 64, 48, 96)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 64, 48, 96)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 64, 48, 96)   36928       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 64, 48, 96)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_1 (TensorFlowOp [(None, 64, 48, 96)] 0           re_lu_2[0][0]                    \n",
      "                                                                 bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 64, 48, 96)   0           tf_op_layer_add_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 128, 24, 48)  73856       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 128, 24, 48)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 128, 24, 48)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 128, 24, 48)  8320        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 128, 24, 48)  147584      re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 128, 24, 48)  512         res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 128, 24, 48)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp [(None, 128, 24, 48) 0           bn3a_branch1[0][0]               \n",
      "                                                                 bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 128, 24, 48)  0           tf_op_layer_add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 128, 24, 48)  147584      re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 128, 24, 48)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 128, 24, 48)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 128, 24, 48)  147584      re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 128, 24, 48)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(None, 128, 24, 48) 0           re_lu_6[0][0]                    \n",
      "                                                                 bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 128, 24, 48)  0           tf_op_layer_add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 256, 12, 24)  295168      re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 256, 12, 24)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 256, 12, 24)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 256, 12, 24)  33024       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 256, 12, 24)  590080      re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 256, 12, 24)  1024        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 256, 12, 24)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_4 (TensorFlowOp [(None, 256, 12, 24) 0           bn4a_branch1[0][0]               \n",
      "                                                                 bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 256, 12, 24)  0           tf_op_layer_add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 256, 12, 24)  590080      re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 256, 12, 24)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 256, 12, 24)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 256, 12, 24)  590080      re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 256, 12, 24)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_5 (TensorFlowOp [(None, 256, 12, 24) 0           re_lu_10[0][0]                   \n",
      "                                                                 bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 256, 12, 24)  0           tf_op_layer_add_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 300, 12, 24)  691500      re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 300, 12, 24)  1200        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 300, 12, 24)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 300, 12, 24)  77100       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 300, 12, 24)  810300      re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 300, 12, 24)  1200        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 300, 12, 24)  1200        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_6 (TensorFlowOp [(None, 300, 12, 24) 0           bn5a_branch1[0][0]               \n",
      "                                                                 bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 300, 12, 24)  0           tf_op_layer_add_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 300, 12, 24)  810300      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 300, 12, 24)  1200        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 300, 12, 24)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 300, 12, 24)  810300      re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 300, 12, 24)  1200        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_7 (TensorFlowOp [(None, 300, 12, 24) 0           re_lu_14[0][0]                   \n",
      "                                                                 bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 300, 12, 24)  0           tf_op_layer_add_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "permute_feature (Permute)       (None, 24, 12, 300)  0           re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_feature (Reshape)       (None, 24, 3600)     0           permute_feature[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24, 512)      8423424     flatten_feature[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "td_dense (TimeDistributed)      (None, 24, 36)       18468       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 24, 36)       0           td_dense[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,432,480\n",
      "Trainable params: 14,424,872\n",
      "Non-trainable params: 7,608\n",
      "__________________________________________________________________________________________________\n",
      "2022-04-08 17:28:33,858 [INFO] __main__: Number of images in the training dataset:\t   111\n",
      "2022-04-08 17:28:33,858 [INFO] __main__: Number of images in the validation dataset:\t   110\n",
      "Epoch 1/24\n",
      "3/4 [=====================>........] - ETA: 3s - loss: 0.8508 3d259a6be299:48:74 [0] NCCL INFO Bootstrap : Using lo:127.0.0.1<0>\n",
      "3d259a6be299:48:74 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "3d259a6be299:48:74 [0] NCCL INFO NET/IB : No device found.\n",
      "3d259a6be299:48:74 [0] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1<0> [1]eth0:172.17.0.3<0>\n",
      "3d259a6be299:48:74 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.9.9+cuda11.3\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 00/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 01/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 02/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 03/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 04/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 05/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 06/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 07/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 08/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 09/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 10/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 11/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 12/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 13/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 14/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 15/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 16/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 17/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 18/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 19/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 20/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 21/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 22/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 23/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 24/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 25/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 26/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 27/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 28/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 29/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 30/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Channel 31/32 :    0\n",
      "3d259a6be299:48:74 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "3d259a6be299:48:74 [0] NCCL INFO Connected all rings\n",
      "3d259a6be299:48:74 [0] NCCL INFO Connected all trees\n",
      "3d259a6be299:48:74 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "3d259a6be299:48:74 [0] NCCL INFO comm 0x7ff7cf3b1ef0 rank 0 nranks 1 cudaDev 0 busId 1000 - Init COMPLETE\n",
      "4/4 [==============================] - 13s 3s/step - loss: 1.1311\n",
      "Epoch 2/24\n",
      "4/4 [==============================] - 1s 282ms/step - loss: 1.4144\n",
      "Epoch 3/24\n",
      "4/4 [==============================] - 1s 188ms/step - loss: 1.1537\n",
      "Epoch 4/24\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 0.9609\n",
      "Epoch 5/24\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 1.0785\n",
      "Epoch 00005: saving model to /workspace/tao-experiments/lprnet/experiments/experiment_dir_unpruned/weights/lprnet_epoch-05.tlt\n",
      "\n",
      "\n",
      "*******************************************\n",
      "Accuracy: 103 / 110  0.9363636363636364\n",
      "*******************************************\n",
      "\n",
      "\n",
      "4/4 [==============================] - 15s 4s/step - loss: 0.8934\n",
      "Epoch 6/24\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.8481\n",
      "Epoch 7/24\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 0.6004\n",
      "Epoch 8/24\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.6699\n",
      "Epoch 9/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.6432\n",
      "Epoch 10/24\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.6400\n",
      "Epoch 00010: saving model to /workspace/tao-experiments/lprnet/experiments/experiment_dir_unpruned/weights/lprnet_epoch-10.tlt\n",
      "\n",
      "\n",
      "*******************************************\n",
      "Accuracy: 104 / 110  0.9454545454545454\n",
      "*******************************************\n",
      "\n",
      "\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.7054\n",
      "Epoch 11/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.6054\n",
      "Epoch 12/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.5520\n",
      "Epoch 13/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.5404\n",
      "Epoch 14/24\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.5672\n",
      "Epoch 15/24\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.4526\n",
      "Epoch 00015: saving model to /workspace/tao-experiments/lprnet/experiments/experiment_dir_unpruned/weights/lprnet_epoch-15.tlt\n",
      "\n",
      "\n",
      "*******************************************\n",
      "Accuracy: 104 / 110  0.9454545454545454\n",
      "*******************************************\n",
      "\n",
      "\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.5801\n",
      "Epoch 16/24\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.6151\n",
      "Epoch 17/24\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.4585\n",
      "Epoch 18/24\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.4040\n",
      "Epoch 19/24\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.4421\n",
      "Epoch 20/24\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.5170\n",
      "Epoch 00020: saving model to /workspace/tao-experiments/lprnet/experiments/experiment_dir_unpruned/weights/lprnet_epoch-20.tlt\n",
      "\n",
      "\n",
      "*******************************************\n",
      "Accuracy: 104 / 110  0.9454545454545454\n",
      "*******************************************\n",
      "\n",
      "\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.4441\n",
      "Epoch 21/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.6411\n",
      "Epoch 22/24\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 0.3829\n",
      "Epoch 23/24\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.4125\n",
      "Epoch 24/24\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 0.8393\n",
      "Epoch 00024: saving model to /workspace/tao-experiments/lprnet/experiments/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt\n",
      "4/4 [==============================] - 2s 452ms/step - loss: 0.6469\n",
      "\n",
      "\n",
      "*******************************************\n",
      "Accuracy: 104 / 110  0.9454545454545454\n",
      "*******************************************\n",
      "\n",
      "\n",
      "2022-04-08 12:29:34,665 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "print(\"For multi-GPU, change --gpus based on your machine.\")\n",
    "!tao lprnet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
    "                  -e $INT_SPECS_DIR/tutorial_spec.txt \\\n",
    "                  -r $INT_MAIN_DIR/experiments/experiment_dir_unpruned \\\n",
    "                  -k $KEY \\\n",
    "                  -m $INT_MAIN_DIR/experiments/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0/us_lprnet_baseline18_trainable.tlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "To resume training from a checkpoint, set the -m option to be the .tlt you want\n",
    "to resume from and --initial_epochs to be the epoch index of the resumed\n",
    "checkpoint\n",
    "\"\"\")\n",
    "# !tao lprnet train --gpu_index=$GPU_INDEX \\\n",
    "#                   -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "#                   -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
    "#                   -k $KEY \\\n",
    "#                   -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-01.tlt\n",
    "#                   --initial_epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/experiment_dir_unpruned/weights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao lprnet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "                     -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n",
    "                     -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inferences <a class=\"anchor\" id=\"head-5\"></a>\n",
    "In this section, we run the lprnet inference tool to generate inferences on the trained models and print the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "!tao lprnet inference --gpu_index=$GPU_INDEX -i $DATA_DOWNLOAD_DIR/val/image \\\n",
    "                      -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n",
    "                      -k $KEY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy! <a class=\"anchor\" id=\"head-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export in FP32 mode. \n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export \n",
    "!tao lprnet export --gpu_index=$GPU_INDEX\n",
    "                   -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n",
    "                   -k $KEY \\\n",
    "                   -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "                   -o $USER_EXPERIMENT_DIR/export/lprnet_epoch-24.etlt \\\n",
    "                   --data_type fp32 \\\n",
    "                   --engine_file $USER_EXPERIMENT_DIR/export/lprnet_epoch-24.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the tensorrt engine accuracy on the validation dataset\n",
    "!tao lprnet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "                     -m $USER_EXPERIMENT_DIR/export/lprnet_epoch-24.engine \\\n",
    "                     --trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if etlt model is correctly saved.\n",
    "!ls -l $LOCAL_EXPERIMENT_DIR/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify engine generation using the `tao-converter` utility included with the docker.\n",
    "\n",
    "The `tao-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tao-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The tao-converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the tao-converter for jetson from the dev zone link [here](https://developer.nvidia.com/tao-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, please refer to [deepstream dev guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lth $LOCAL_EXPERIMENT_DIR/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorRT engine(FP16).\n",
    "# Please use -p option to set the min_shape / opt_shape / max_shape for dynmaic batch engine.\n",
    "!tao converter $USER_EXPERIMENT_DIR/export/lprnet_epoch-24.etlt \\\n",
    "                   -k $KEY \\\n",
    "                   -p image_input,1x3x48x96,4x3x48x96,16x3x48x96 \\\n",
    "                   -t fp16 \\\n",
    "                   -e $USER_EXPERIMENT_DIR/export/lprnet_epoch-24_dynamic_batch.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported engine:')\n",
    "print('------------')\n",
    "!ls -lh $LOCAL_EXPERIMENT_DIR/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify the deployed model <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "Verify the converted engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running evaluation on validation dataset\n",
    "!tao lprnet evaluate  --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n",
    "                      -m $USER_EXPERIMENT_DIR/export/lprnet_epoch-24_dynamic_batch.engine \\\n",
    "                      --trt        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobia_lpr",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
